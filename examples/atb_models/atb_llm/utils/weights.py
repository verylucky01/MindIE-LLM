# Copyright 2022 Hugging Face 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at 
#     http://www.apache.org/licenses/LICENSE-2.0 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Implement part of class Weights based on text-generation-inference
# Copyright (c) Huawei Technologies Co., Ltd. 2024-2025. All rights reserved.
# MindIE is licensed under Mulan PSL v2.
# You can use this software according to the terms and conditions of the Mulan PSL v2.
# You may obtain a copy of Mulan PSL v2 at:
#          http://license.coscl.org.cn/MulanPSL2
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
# See the Mulan PSL v2 for more details.

import json
import math
import os
from typing import List, Dict, Optional, Tuple
from enum import Enum

import acl
import torch
import torch_npu
from safetensors import safe_open, SafetensorError

from .hub import weight_files
from .log import logger, print_log
from .quantize.quant_type import QuantType, LinearTypeV2, QUANTIZE_DESC_REQUIRED_LIST
from . import file_utils
from .layers.linear.linear_utils import LinearUtils

QUANTIZE_DTYPE_LIST = [torch.int8, torch.int32, torch.int64]


class ProcessGroupType(Enum):
    ATTN = "attn"
    MLP = "mlp"
    MOE_TP = "moe_tp"
    MOE_EP = "moe_ep"
    MOE_DP = "moe_dp"
    LM_HEAD = "lm_head"
    ATTN_O = "attn_o"
    DENSE_TP = "dense_tp"


class Weights:
    def __init__(
            self,
            model_name_or_path,
            device,
            dtype,
            process_group,
            quantize=None,
            is_gqa: Optional[bool] = False,
            extension: Optional[str] = ".safetensors",
            aliases: Optional[Dict[str, List[str]]] = None,
            mapping=None,
            **kwargs
    ):
        self.sharded = False
        if quantize in [QuantType.W8A8SC, QuantType.W16A16SC]:
            model_name_or_path = os.path.join(model_name_or_path,
                                              f'part{process_group.rank()}-of-{process_group.size()}'
            )
            model_name_or_path = file_utils.standardize_path(model_name_or_path, check_link=False)
            file_utils.check_path_permission(model_name_or_path)
            self.filenames = weight_files(model_name_or_path, extension=extension)
        
        model_sharded_metadata_path = os.path.join(model_name_or_path, "model_sharded_metadata.json")
        if os.path.exists(model_sharded_metadata_path):
            self.sharded = True
            self.filenames = []
            sub_folders = []
            if mapping is not None:
                sub_folders.append(os.path.join(model_name_or_path,
                    f'model-moe-tp-{str(mapping.moe_tp.rank).zfill(3)}-ep-{str(mapping.moe_ep.rank).zfill(3)}'))
                sub_folders.append(os.path.join(model_name_or_path, f'model-dense-tp-{str(mapping.attn_tp.rank).zfill(3)}'))
                sub_folders.append(os.path.join(model_name_or_path, f'model-attn-tp-{str(mapping.attn_tp.rank).zfill(3)}'))
                sub_folders.append(os.path.join(model_name_or_path, f'model-{str(mapping.rank).zfill(3)}'))
                sub_folders.append(os.path.join(model_name_or_path, 'model-norm'))
            for sub_folder in sub_folders:
                path = os.path.join(model_name_or_path, sub_folder)
                path = file_utils.standardize_path(path, check_link=False)
                file_utils.check_path_permission(path)
                self.filenames += weight_files(path, extension=extension)
        else:
            self.filenames = weight_files(model_name_or_path, extension=extension)

        self.dimfiles = model_name_or_path
        self.quantize = quantize
        routing = self.load_routing(process_group)
        self.weight_dims = {}
        if aliases is None:
            aliases = {}
        self.aliases = aliases
        self.routing = routing
        self.device = device
        self.dtype = dtype
        self.mapping = mapping
        self.is_gqa = is_gqa
        self.process_group = process_group
        self.process_group_dict = {}
        self.init_process_group(process_group, mapping)
        self._handles = {}
        self.gptq_bits = None
        self.gptq_groupsize = None
        self.quant_desc = None
        self.expert_routing_map = {}

        self.init_quant_params(quantize, model_name_or_path)

    @classmethod
    def _nz2nd(cls, nz_tensor):
        temp_tensor = torch.zeros_like(nz_tensor).npu()
        torch.npu.config.allow_internal_format = True
        torch_npu.npu_format_cast_(temp_tensor, 29)
        torch.npu.synchronize()
        acl.rt.memcpy(temp_tensor.untyped_storage().data_ptr(),
                        temp_tensor.untyped_storage().nbytes(),
                        nz_tensor.untyped_storage().data_ptr(),
                        nz_tensor.untyped_storage().nbytes(), 1)
        torch_npu.npu_format_cast_(temp_tensor, 2)
        return temp_tensor
    
    @classmethod
    def _unpermute(cls, weight, dim):
        original_shape = weight.shape
        if dim < 0:
            dim += len(original_shape)
        weight = weight.view(*original_shape[:dim], 16, 2, 128, *original_shape[dim + 1:])
        weight = weight.transpose(dim, dim + 1).contiguous()
        weight = weight.view(*original_shape[:dim], -1, *original_shape[dim + 1:])
        return weight.contiguous()

    def release_file_handler(self):
        del self._handles
        self._handles = {}

    def load_routing(self, process_group):
        routing = {}
        for filename in self.filenames:
            filename = file_utils.standardize_path(str(filename), check_link=False)
            file_utils.check_path_permission(filename)
            with safe_open(filename, framework="pytorch") as f:
                for k in f.keys():
                    if k in routing:
                        error_msg = (
                            f"routing weight key error in process {process_group.rank()}:\n"
                            f"  - Key (Weight): {k}\n"
                            f"  - Existing file: {routing[k]}\n"
                            f"  - New file: {filename}\n"
                        )
                        print_log(process_group.rank(), logger.error, error_msg, need_filter=True)
                        raise AssertionError(error_msg)
                    routing[k] = filename
        return routing

    def load_dims(self, process_group):
        outdim_file = os.path.join(self.dimfiles, "quant_model_description.json")
        outdim_file = file_utils.standardize_path(outdim_file, check_link=False)
        file_utils.check_path_permission(outdim_file)
        with file_utils.safe_open(outdim_file, "r", encoding="utf-8", check_link=False) as f:
            quant_model_description = json.load(f)
        sparse_outdims = {}
        for key, value in quant_model_description.items():
            if key.endswith(".outdim"):
                sparse_outdims[key] = value
        return sparse_outdims
        
    def init_process_group(self, process_group, mapping):
        self.process_group_dict = {
            ProcessGroupType.ATTN: process_group,
            ProcessGroupType.MLP: process_group,
            ProcessGroupType.LM_HEAD: process_group,
            ProcessGroupType.DENSE_TP: process_group
        }
        if mapping is not None:
            from .dist import FakeGroup
            self.process_group_dict[ProcessGroupType.MOE_DP] = FakeGroup(0, 1)
            if mapping.has_dp() or mapping.has_attn_cp():
                self.process_group_dict[ProcessGroupType.ATTN] = \
                    FakeGroup(mapping.attn_tp.rank, mapping.attn_tp.group_size)
                self.process_group_dict[ProcessGroupType.MLP] = \
                    FakeGroup(mapping.mlp_tp.rank, mapping.mlp_tp.group_size)
                self.process_group_dict[ProcessGroupType.LM_HEAD] = \
                    FakeGroup(mapping.lm_head_tp.rank, mapping.lm_head_tp.group_size)
                self.switch_process_group(ProcessGroupType.ATTN)
            if mapping.enable_dense_tp:
                self.process_group_dict[ProcessGroupType.DENSE_TP] = \
                    FakeGroup(mapping.dense_tp.rank, mapping.dense_tp.group_size)
            if mapping.has_moe_ep():
                self.process_group_dict[ProcessGroupType.MOE_EP] = \
                    FakeGroup(mapping.moe_tp.rank, mapping.moe_tp.group_size)
            if mapping.has_moe_tp():
                self.process_group_dict[ProcessGroupType.MOE_TP] = \
                    FakeGroup(mapping.moe_tp.rank, mapping.moe_tp.group_size)
            if mapping.has_attn_o_proj_tp():
                self.process_group_dict[ProcessGroupType.ATTN_O] = \
                    FakeGroup(mapping.attn_o_proj_tp.rank, mapping.attn_o_proj_tp.group_size)
            if mapping.has_pp():
                self.process_group_dict[ProcessGroupType.MLP] = FakeGroup(mapping.pp.tp.rank, \
                    mapping.pp.tp.group_size)
                self.switch_process_group(ProcessGroupType.MLP)


    def switch_process_group(self, process_group_type: ProcessGroupType):
        if process_group_type not in self.process_group_dict.keys():
            raise ValueError(f"Process group name {process_group_type} is invalid, please check!")
        self.process_group = self.process_group_dict[process_group_type]

    def get_linear_quant_type(self, key):
        if self.quant_desc is None:
            return LinearTypeV2.FLOAT16 if self.dtype == torch.float16 else LinearTypeV2.BFLOAT16
        if self.quant_desc.get(key, LinearTypeV2.INVALID) == "FLOAT":
            return LinearTypeV2.FLOAT16 if self.dtype == torch.float16 else LinearTypeV2.BFLOAT16
        return LinearTypeV2[self.quant_desc.get(key, LinearTypeV2.INVALID)]

    def correct_tensor_dtype(self, tensor, tensor_name):
        if tensor_name.endswith("deq_scale") and self.dtype == torch.bfloat16:
            # BF16场景下deq_scale字段的值为FP32
            return tensor
        if tensor_name.endswith("scale_bias"):
            return tensor
        if tensor_name.endswith("e_score_correction_bias"):
            tensor = tensor - torch.min(tensor)
        if tensor.dtype not in [torch.int8, torch.int32, torch.int64]:
            tensor = tensor.to(dtype=self.dtype)
        return tensor

    def init_quant_params(self, quantize, model_name_or_path):
        if quantize in QUANTIZE_DESC_REQUIRED_LIST:
            self._set_quant_params(model_name_or_path)
            # Version of MsModelSlim
            LinearUtils.quant_version = self.quant_desc.get("version", "0.0.0")

    def get_filename(self, tensor_name: str) -> (str, str):
        filename = self.routing.get(tensor_name, None)
        if filename is None:
            aliases = self.aliases.get(tensor_name, [])
            for alias in aliases:
                filename = self.routing.get(alias, None)
                if filename is not None:
                    return str(filename), alias
            raise AssertionError(f"weight {tensor_name} does not exist")
        return str(filename), tensor_name

    def get_shape(self, tensor_name: str):
        return self._get_slice(tensor_name).get_shape()

    def get_tensor(self, tensor_name: str, ignore_tensor_correction=False):
        filename, tensor_name = self.get_filename(tensor_name)
        f = self._get_handle(filename)
        tensor = f.get_tensor(tensor_name)
        if ignore_tensor_correction:
            return tensor
        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_whole_tensor(self, tensor_name: str, dim: int):
        slice_ = self._get_slice(tensor_name)

        start = 0
        stop = slice_.get_shape()[dim]

        if dim == 0:
            tensor = slice_[start:stop]
        elif dim == 1:
            tensor = slice_[:, start:stop]
        else:
            logger.error("Let's make that generic when needed")
            raise AssertionError
        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_partial_sharded_mid_dim(self, tensor_name: str, dim: int, index: int = 1, gqa_size: int = 1):

        world_size = self.process_group.size()
        rank = self.process_group.rank()

        slice_ = self._get_slice(tensor_name)
        size = slice_.get_shape()[dim]

        block_size = size // 16
        start = block_size * index + rank * block_size // world_size
        stop = block_size * index + (rank + 1) * block_size // world_size

        if dim == 0:
            tensor = slice_[start:stop, :]
        elif dim == 1:
            tensor = slice_[:, start:stop]
        else:
            logger.error("Let's make that generic when needed")
            raise AssertionError
        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_partial_sharded(self, tensor_name: str, dim: int, gqa_size: int = 1):
        world_size = self.process_group.size()
        rank = self.process_group.rank()

        slice_ = self._get_slice(tensor_name)
        size = slice_.get_shape()[dim]
        group_size = size // gqa_size
        if group_size >= world_size:
            block_size = size // world_size
            start = rank * block_size
            stop = (rank + 1) * block_size
        else:
            block_size = gqa_size
            start = (rank // (world_size // group_size)) * block_size
            stop = ((rank // (world_size // group_size)) + 1) * block_size

        if "c_attn.bias" in tensor_name:
            b = slice_[:]
            single_size = b.shape[0] // 3
            head_size = 128
            head_num = single_size // head_size
            rank_heads = math.ceil(head_num / world_size)
            if rank != world_size - 1:
                start = rank * (rank_heads * head_size)
                stop = (rank + 1) * (rank_heads * head_size)
                bq = slice_[start:stop]
                bk = slice_[start + single_size:stop + single_size]
                bv = slice_[start + 2 * single_size:stop + 2 * single_size]
            else:
                # last rank
                start = rank * (rank_heads * head_size)
                stop = head_num * head_size
                bq = slice_[start:stop]
                bk = slice_[start + single_size:stop + single_size]
                bv = slice_[start + 2 * single_size:stop + 2 * single_size]
            b_ = torch.cat([bq, bk, bv], dim=0)
            return b_

        if dim == 0:
            tensor = slice_[start:stop]
        elif dim == 1:
            tensor = slice_[:, start:stop]
        else:
            logger.error("Let's make that generic when needed")
            raise AssertionError
        return self.correct_tensor_dtype(tensor, tensor_name)
    
    def get_partial_sharded_padding_gqa(self, tensor_name: str, dim: int, gqa_size=1):
        world_size = self.process_group.size()
        rank = self.process_group.rank()

        slice_ = self.get_tensor(tensor_name)
        size = slice_.shape[dim]

        head_num = size // gqa_size
        block_head_num = (head_num + world_size - 1) // world_size

        block_size = block_head_num * gqa_size
        block_size_padding = (block_head_num - 1) * gqa_size

        group_rank = world_size // (block_head_num * world_size - head_num)
        group_size = (group_rank - 1) * block_size + block_size_padding

        if rank % group_rank == 0:
            start = (rank // group_rank) * group_size
            indices = torch.arange(start, start + block_size_padding).to(torch.int32)
            tensor = torch.index_select(slice_, dim, indices)
            if len(tensor.shape) == 1:
                tensor_zeros = torch.zeros(size=(block_size,), dtype=tensor.dtype, device=tensor.device)
                tensor_zeros[:tensor.shape[0]] = tensor
                tensor = tensor_zeros
            else:
                dim0, dim1 = tensor.shape
                if dim == 0:
                    dim0 = block_size
                else:
                    dim1 = block_size
                tensor_zeros = torch.zeros(size=(dim0, dim1), dtype=tensor.dtype, device=tensor.device)
                tensor_zeros[:tensor.shape[0], :tensor.shape[1]] = tensor
                tensor = tensor_zeros
        else:
            start = (rank // group_rank) * group_size + (
                        (rank % group_rank - 1) * block_size + block_size_padding)
            indices = torch.arange(start, start + block_size).to(torch.int32)
            tensor = torch.index_select(slice_, dim, indices)

        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_partial_sharded_padding(self, tensor_name: str, dim: int, gqa_size=1):
        world_size = self.process_group.size()
        rank = self.process_group.rank()

        slice_ = self._get_slice(tensor_name)
        size = slice_.get_shape()[dim]

        head_num = size // gqa_size
        block_head_num = (head_num + world_size - 1) // world_size

        block_size = block_head_num * gqa_size

        start = rank * block_size
        stop = (rank + 1) * block_size

        if rank != world_size - 1:
            if dim == 0:
                tensor = slice_[start:stop]
            elif dim == 1:
                tensor = slice_[:, start:stop]
            else:
                logger.error("Let's make that generic when needed")
                raise AssertionError
        else:
            if dim == 0:
                tensor = slice_[start:]
            elif dim == 1:
                tensor = slice_[:, start:]
            else:
                logger.error("Let's make that generic when needed")
                raise AssertionError
        
        if len(tensor.shape) == 1:
            tensor_zeros = torch.zeros(size=(block_size,), dtype=tensor.dtype, device=tensor.device)
            tensor_zeros[:tensor.shape[0]] = tensor
            tensor = tensor_zeros
        else:
            dim0, dim1 = tensor.shape
            if dim == 0:
                dim0 = block_size
            else:
                dim1 = block_size
            tensor_zeros = torch.zeros(size=(dim0, dim1), dtype=tensor.dtype, device=tensor.device)
            tensor_zeros[:tensor.shape[0], :tensor.shape[1]] = tensor
            tensor = tensor_zeros

        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_sharded(self, tensor_name: str, dim: int, gqa_size: int = 1):
        slice_ = self._get_slice(tensor_name)
        world_size = self.process_group.size()
        size = slice_.get_shape()[dim]
        if (size // gqa_size) % world_size == 0 or world_size % (size // gqa_size) == 0:
            return self.get_partial_sharded(tensor_name, dim, gqa_size)
        else:
            if self.is_gqa and gqa_size != 1:
                # qkvo 分组padding
                return self.get_partial_sharded_padding_gqa(tensor_name, dim, gqa_size)
            else:
                return self.get_partial_sharded_padding(tensor_name, dim, gqa_size)

    def get_per_tensor_sharded(self, prefixes, dim, tensor_name):
        tensor = torch.cat(
            [self.get_whole_tensor(f"{p}.{tensor_name}", dim=0) for p in prefixes], dim=dim
        )
        if torch.allclose(tensor, tensor[0]):
            tensor = tensor[:1]
        else:
            raise ValueError(f"`{tensor_name}` are not equal: {tensor}")
        return tensor
    
    def get_tensor_col_packed_qkv_mha(self, tensor_name: str, head_size: int = None, dim=0):
        slice_ = self._get_slice(tensor_name)
        total_size = slice_.get_shape()[-1 if dim == 1 else 0]
        if total_size % 3 != 0:
            raise ValueError("Prepacked qkv is not divisible by 3")
        single_size = total_size // 3
        world_size = self.process_group.size()
        rank = self.process_group.rank()
        if dim == 1:
            if head_size is None:
                if single_size % world_size != 0:
                    raise RuntimeError(f"Prepacked qkv cannot be sharded across {world_size} shards")
                block_size = single_size // world_size
                start = rank * block_size
                stop = (rank + 1) * block_size
                if len(slice_.get_shape()) <= 1:
                    q = slice_[start:stop]
                    k = slice_[start + single_size:stop + single_size]
                    v = slice_[start + 2 * single_size:stop + 2 * single_size]
                    tensor = torch.cat([q, k, v], dim=0)
                else:
                    q = slice_[:, start:stop]
                    k = slice_[:, start + single_size:stop + single_size]
                    v = slice_[:, start + 2 * single_size:stop + 2 * single_size]
                    tensor = torch.cat([q, k, v], dim=1)
            else:
                raise ValueError("qkv are not supported")
        else:
            if head_size is None:
                if single_size % world_size != 0:
                    raise RuntimeError(f"Prepacked qkv cannot be sharded across {world_size} shards")
                block_size = single_size // world_size
                start = rank * block_size
                stop = (rank + 1) * block_size
                q = slice_[start:stop]
                k = slice_[start + single_size:stop + single_size]
                v = slice_[start + 2 * single_size:stop + 2 * single_size]
                tensor = torch.cat([q, k, v], dim=0)
            else:
                head_num = single_size // head_size
                rank_heads = math.ceil(head_num / world_size)
                if rank != world_size - 1:
                    start = rank * (rank_heads * head_size)
                    stop = (rank + 1) * (rank_heads * head_size)
                    q = slice_[start:stop]
                    k = slice_[start + single_size:stop + single_size]
                    v = slice_[start + 2 * single_size:stop + 2 * single_size]
                    tensor = torch.cat([q, k, v], dim=0)
                else:
                    # last rank
                    start = rank * (rank_heads * head_size)
                    stop = head_num * head_size
                    q = slice_[start:stop]
                    k = slice_[start + single_size:stop + single_size]
                    v = slice_[start + 2 * single_size:stop + 2 * single_size]

                    # padding
                    q_zero = torch.zeros(size=(rank_heads * head_size, slice_.get_shape()[1]))
                    k_zero = torch.zeros(size=(rank_heads * head_size, slice_.get_shape()[1]))
                    v_zero = torch.zeros(size=(rank_heads * head_size, slice_.get_shape()[1]))
                    q_zero[:q.shape[0], :q.shape[1]] = q
                    k_zero[:k.shape[0], :k.shape[1]] = k
                    v_zero[:v.shape[0], :v.shape[1]] = v
                    tensor = torch.cat([q_zero, k_zero, v_zero], dim=0)
        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_tensor_col_packed_qkv_gqa(self, tensor_name: str, num_heads, num_kv_heads):
        slice_ = self.get_tensor(tensor_name)
        total_size = slice_.shape[0]
        if total_size % (num_heads + num_kv_heads * 2) != 0:
            raise AssertionError("Prepacked qkv is not divisible by q,k,v")
        q_single_size = total_size * num_heads // (num_heads + num_kv_heads * 2)
        kv_single_size = total_size * num_kv_heads // (num_heads + num_kv_heads * 2)
        world_size = self.process_group.size()
        rank = self.process_group.rank()
        if q_single_size % world_size != 0:
            raise AssertionError(f"Prepacked qkv cannot be sharded across {world_size} shards")
        query_layer, key_layer, value_layer = slice_.split((q_single_size, kv_single_size, kv_single_size), dim=0)
        kv_tp_size = min(world_size, num_kv_heads)
        query_list = torch.chunk(query_layer, world_size, dim=0)
        key_list = torch.chunk(key_layer, kv_tp_size, dim=0)
        value_list = torch.chunk(value_layer, kv_tp_size, dim=0)
        tensor = torch.cat([query_list[rank],
                            key_list[rank * kv_tp_size // world_size],
                            value_list[rank * kv_tp_size // world_size]], dim=0)
        return self.correct_tensor_dtype(tensor, tensor_name)
    
    def get_tensor_col_packed_kv_mha(self, tensor_name: str, hiden_size, head_size: int = None):
        slice_ = self._get_slice(tensor_name)
        total_size = slice_.get_shape()[0]
        if total_size % 2 != 0:
            raise ValueError("Prepacked qkv is not divisible by 2")
        single_size = total_size // 2
        world_size = self.process_group.size()
        rank = self.process_group.rank()

        if head_size is None:
            raise RuntimeError("head_size is neccessary")
        else:
            head_num = single_size // head_size
            rank_heads = math.ceil(head_num / world_size)
            start = rank * (rank_heads * head_size * 2)
            stop = (rank + 1) * (rank_heads * head_size * 2)
            kv = slice_[start:stop]
            kv_new = kv.reshape(rank_heads, 2, head_size, -1)
            k, v = torch.chunk(kv_new, 2, dim=1)
            if len(slice_.get_shape()) == 1:
                k = k.reshape(head_size * rank_heads)
                v = v.reshape(head_size * rank_heads)
            else:
                k = k.reshape(head_size * rank_heads, hiden_size)
                v = v.reshape(head_size * rank_heads, hiden_size)
            tensor = torch.cat([k, v], dim=0)
        
        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_w8a8sc_weight(self, prefix: str):
        qweight = self.get_tensor(f"{prefix}.weight")
        if qweight.dtype in [torch.float16, torch.bfloat16]:
            return qweight
        deq_scale = self.get_tensor(f"{prefix}.deq_scale")
        quant_bias = self.get_tensor(f"{prefix}.quant_bias")
        input_scale = self.get_tensor(f"{prefix}.input_scale")
        input_offset = self.get_tensor(f"{prefix}.input_offset")
        index = self.get_tensor(f"{prefix}.index")
        weight = (qweight, deq_scale, quant_bias, input_scale, input_offset, index)
        return weight

    def get_weights_col_packed_kv(self, prefix: str, quantize: str, hidden_size, head_size, num_kv_heads=None):
        if quantize in [QuantType.W8A8, QuantType.W8A8S]:
            qweight = self.get_tensor_col_packed_kv_mha(f"{prefix}.weight", hidden_size, head_size)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = self.get_tensor_col_packed_kv_mha(f"{prefix}.deq_scale", hidden_size, head_size)
            quant_bias = self.get_tensor_col_packed_kv_mha(f"{prefix}.quant_bias", hidden_size, head_size)
            input_scale = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_scale')
            input_offset = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_offset')
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset)
        elif quantize in [QuantType.W4A16, QuantType.W8A16, QuantType.W8A8_DYNAMIC]:
            qweight = self.get_tensor_col_packed_kv_mha(f"{prefix}.weight", hidden_size, head_size)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_tensor_col_packed_kv_mha(f"{prefix}.weight_scale", hidden_size, head_size)
            weight_offset = self.get_tensor_col_packed_kv_mha(f"{prefix}.weight_offset", hidden_size, head_size)
            weight = (qweight, weight_scale, weight_offset)
        elif quantize == QuantType.W8A8SC:
            return self.get_w8a8sc_weight(prefix)
        else:
            weight = self.get_tensor_col_packed_kv_mha(f"{prefix}.weight", hidden_size, head_size)
        return weight

    def get_tensor_col_packed_qkv(self, tensor_name: str, hidden_size, num_heads, num_kv_heads=None, dim=0):
        if not num_kv_heads:
            num_kv_heads = num_heads
        if num_heads == num_kv_heads:
            if num_heads % self.process_group.size() == 0:
                return self.get_tensor_col_packed_qkv_mha(tensor_name, dim=dim)
            else:
                return self.get_tensor_col_packed_qkv_mha(tensor_name, hidden_size // num_heads, dim=dim)
        else:
            return self.get_tensor_col_packed_qkv_gqa(tensor_name, num_heads, num_kv_heads)

    def get_weights_col_packed_qkv(self, prefix: str, quantize: str, hidden_size, num_heads, num_kv_heads=None, dim=0):
        if quantize in [QuantType.W8A8, QuantType.W8A8S]:
            qweight = self.get_tensor_col_packed_qkv(f"{prefix}.weight", hidden_size, num_heads, num_kv_heads)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = self.get_tensor_col_packed_qkv(f"{prefix}.deq_scale", hidden_size, num_heads, num_kv_heads)
            quant_bias = self.get_tensor_col_packed_qkv(f"{prefix}.quant_bias", hidden_size, num_heads, num_kv_heads)
            input_scale = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_scale')
            input_offset = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_offset')
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset)
        elif quantize in [QuantType.W4A16, QuantType.W8A16, QuantType.W8A8_DYNAMIC]:
            weight_effective_hidden_size = hidden_size
            # The new version of int4 weight is packed by default.
            # Thus, when the attention head axis is partitioned, head size should be cut in half.
            if quantize == QuantType.W4A16 and self.quant_desc.get("group_size", 0) > 0 \
                and self.quant_desc.get("version", "0.0.0") != "0.0.0":
                weight_effective_hidden_size = (hidden_size + 1) // 2  # round up
            qweight = self.get_tensor_col_packed_qkv(
                f"{prefix}.weight", weight_effective_hidden_size, num_heads, num_kv_heads)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_tensor_col_packed_qkv(f"{prefix}.weight_scale", hidden_size, num_heads,
                                                          num_kv_heads)
            weight_offset = self.get_tensor_col_packed_qkv(f"{prefix}.weight_offset", hidden_size, num_heads,
                                                           num_kv_heads)
            weight = (qweight, weight_scale, weight_offset)
        elif quantize == QuantType.W8A8SC:
            return self.get_w8a8sc_weight(prefix)
        else:
            weight = self.get_tensor_col_packed_qkv(f"{prefix}.weight", hidden_size, num_heads, num_kv_heads, dim=dim)
        return weight

    def get_tensor_col_packed_mlp(self, tensor_name, head_types=2):
        slice_ = self.get_tensor(tensor_name)
        total_size = slice_.shape[0]
        if total_size % head_types != 0:
            raise AssertionError("Prepacked mlp is not divisible by up,gate")
        up_single_size = total_size // head_types
        gate_single_size = total_size // head_types
        world_size = self.process_group.size()
        rank = self.process_group.rank()
        if up_single_size % world_size != 0:
            raise AssertionError(f"Prepacked mlp cannot be sharded across {world_size} shards")
        gate_layer, up_layer = slice_.split((up_single_size, gate_single_size), dim=0)
        gate_list = torch.chunk(gate_layer, world_size, dim=0)
        up_list = torch.chunk(up_layer, world_size, dim=0)
        tensor = torch.cat([gate_list[rank], up_list[rank]], dim=0)
        return self.correct_tensor_dtype(tensor, tensor_name)

    def get_weights_col_packed_mlp(self, prefix: str, quantize: str):
        if quantize in [QuantType.W8A8, QuantType.W8A8S]:
            qweight = self.get_tensor_col_packed_mlp(f"{prefix}.weight")
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = self.get_tensor_col_packed_mlp(f"{prefix}.deq_scale")
            quant_bias = self.get_tensor_col_packed_mlp(f"{prefix}.quant_bias")
            input_scale = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_scale')
            input_offset = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_offset')
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset)
        elif quantize in [QuantType.W4A16, QuantType.W8A16, QuantType.W8A8_DYNAMIC]:
            qweight = self.get_tensor_col_packed_mlp(f"{prefix}.weight")
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_tensor_col_packed_mlp(f"{prefix}.weight_scale")
            weight_offset = self.get_tensor_col_packed_mlp(f"{prefix}.weight_offset")
            weight = (qweight, weight_scale, weight_offset)
        elif quantize == QuantType.W4A8_DYNAMIC:
            qweight = self.get_tensor_col_packed_mlp(f"{prefix}.weight")
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_tensor_col_packed_mlp(f"{prefix}.weight_scale")
            weight_offset = self.get_tensor_col_packed_mlp(f"{prefix}.weight_offset")
            weight_scale_second = self.get_sharded(f"{prefix}.weight_scale_second")
            weight_offset_second = self.get_sharded(f"{prefix}.weight_offset_second")
            weight = (qweight, weight_scale, weight_offset, weight_scale_second, weight_offset_second)
        elif quantize == QuantType.W8A8SC:
            return self.get_w8a8sc_weight(prefix)
        else:
            weight = self.get_tensor_col_packed_mlp(f"{prefix}.weight")
        return weight
    
    def get_w8a8_pdmix_weight_col(self, prefixes: List[str], dim: int, gqa_size: int = 1):
        qweight = torch.cat(
            [self.get_sharded(f"{p}.weight", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
        )
        if qweight.dtype in [torch.float16, torch.bfloat16]:
            return qweight
        weight_scale = torch.cat(
            [self.get_sharded(f"{p}.weight_scale", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
        )
        weight_offset = torch.cat(
            [self.get_sharded(f"{p}.weight_offset", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
        )
        deq_scale = torch.cat(
            [self.get_sharded(f"{p}.deq_scale", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
        )
        quant_bias = torch.cat(
            [self.get_sharded(f"{p}.quant_bias", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
        )
        input_scale = self.get_per_tensor_sharded(prefixes, dim, 'input_scale')
        input_offset = self.get_per_tensor_sharded(prefixes, dim, 'input_offset')
        weight = (qweight, weight_scale, weight_offset, deq_scale, quant_bias, input_scale, input_offset)
        return weight

    def get_multi_weights_col(self,
                              prefixes: List[str],
                              quantize: str,
                              dim: int,
                              gqa_size: int = 1,  # 1: QK weight is divided by the integer multiple of gqa_size.
                              routing_expert_dim=None):
        if quantize == "gptq":
            try:
                qweight = torch.cat(
                    [self.get_sharded(f"{p}.qweight", dim=1) for p in prefixes], dim=1
                )
            except RuntimeError as err:
                logger.error(
                    "Cannot load `gptq` weight, make sure the model is already quantized"
                )
                raise AssertionError from err

            qzeros = torch.cat(
                [self.get_sharded(f"{p}.qzeros", dim=1) for p in prefixes], dim=1
            )
            scales = torch.cat(
                [self.get_sharded(f"{p}.scales", dim=1) for p in prefixes], dim=1
            )
            w = [self.get_tensor(f"{p}.g_idx") for p in prefixes]
            for w2 in w[1:]:
                torch.testing.assert_close(w2, w[0])
            g_idx = w[0]

            bits, groupsize = self._get_gptq_params()
            weight = (qweight, qzeros, scales, g_idx, bits, groupsize, False)
        elif quantize in [QuantType.W8A8, QuantType.W8A8S]:
            qweight = torch.cat(
                [self.get_sharded(f"{p}.weight", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = torch.cat(
                [self.get_sharded(f"{p}.deq_scale", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            quant_bias = torch.cat(
                [self.get_sharded(f"{p}.quant_bias", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            input_scale = self.get_per_tensor_sharded(prefixes, dim, 'input_scale')
            input_offset = self.get_per_tensor_sharded(prefixes, dim, 'input_offset')
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset)
        elif quantize in [QuantType.W4A16, QuantType.W8A16, QuantType.W8A8_DYNAMIC]:
            weight_effective_head_size = gqa_size
            # The new version of int4 weight is packed by default.
            # Thus, when the attention head axis is partitioned, head size should be cut in half.
            if quantize == QuantType.W4A16 and self.quant_desc.get("group_size", 0) > 0 \
                and self.quant_desc.get("version", "0.0.0") != "0.0.0":
                weight_effective_head_size = (gqa_size + 1) // 2  # round up

            if routing_expert_dim is not None:
                qweight = torch.cat(
                    [self.get_sharded(f"{p}.weight", dim=routing_expert_dim) for p in prefixes],
                    dim=routing_expert_dim
                )
            else:
                qweight = torch.cat(
                    [self.get_sharded(f"{p}.weight", dim=0, gqa_size=weight_effective_head_size) for p in prefixes],
                    dim=dim
                )
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = torch.cat(
                [self.get_sharded(f"{p}.weight_scale", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            weight_offset = torch.cat(
                [self.get_sharded(f"{p}.weight_offset", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            weight = (qweight, weight_scale, weight_offset)
        elif quantize == QuantType.W8A8SC:
            qweight = torch.cat([self.get_tensor(f"{p}.weight") for p in prefixes], dim=dim)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = torch.cat([self.get_tensor(f"{p}.deq_scale") for p in prefixes], dim=dim)
            quant_bias = torch.cat([self.get_tensor(f"{p}.quant_bias") for p in prefixes], dim=dim)
            input_scale = torch.cat([self.get_tensor(f"{p}.input_scale") for p in prefixes], dim=dim)
            input_offset = torch.cat([self.get_tensor(f"{p}.input_offset") for p in prefixes], dim=dim)
            index = torch.cat([self.get_tensor(f"{p}.index") for p in prefixes], dim=dim)
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset, index)
        elif quantize == QuantType.W8A8_PDMIX:
            weight = self.get_w8a8_pdmix_weight_col(prefixes, dim, gqa_size)
        elif quantize == QuantType.W4A8_DYNAMIC:
            qweight = torch.cat(
                [self.get_sharded(f"{p}.weight", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = torch.cat(
                [self.get_sharded(f"{p}.weight_scale", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            weight_bias = torch.cat(
                [self.get_sharded(f"{p}.scale_bias", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
            )
            if self.quant_desc.get("group_size") == 0:
                weight = (qweight, weight_scale, weight_bias)
            else:
                weight_scale_second = torch.cat(
                    [self.get_sharded(f"{p}.weight_scale_second", dim=0, gqa_size=gqa_size) for p in prefixes], dim=dim
                )
                weight = (qweight, weight_scale, weight_scale_second, weight_bias)
        elif quantize == QuantType.W16A16SC:
            qweight = torch.cat([self.get_tensor(f"{p}.weight") for p in prefixes], dim=dim)
            index = torch.cat([self.get_tensor(f"{p}.index") for p in prefixes], dim=dim)
            self.weight_dims = self.load_dims(self.process_group)
            dim_n = self.weight_dims[f"{prefixes[0]}.weight.outdim"]
            if self.weight_dims.get(f"{prefixes[0]}.bias.outdim"):
                quant_bias = self.get_tensor(f"{prefixes[0]}.bias").to(torch.float32)
            else:
                quant_bias = torch.zeros(dim_n, dtype=torch.float32)
            weight = (qweight, index, quant_bias)      
        else:
            w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
            weight = torch.cat(w, dim=dim)
        return weight

    def get_w8a8_pdmix_weight_row(self, prefix: str, gqa_size=1, dim=1):
        qweight = self.get_sharded(f"{prefix}.weight", dim=1, gqa_size=gqa_size)
        if qweight.dtype in [torch.float16, torch.bfloat16]:
            return qweight
        weight_scale = self.get_sharded(f"{prefix}.weight_scale", dim=dim, gqa_size=1)
        weight_offset = self.get_sharded(f"{prefix}.weight_offset", dim=dim, gqa_size=1)
        deq_scale = self.get_tensor(f"{prefix}.deq_scale")
        quant_bias = self.get_tensor(f"{prefix}.quant_bias")
        if self.process_group.rank() == 0:
            quant_bias = quant_bias
        else:
            quant_bias = torch.zeros_like(quant_bias, dtype=quant_bias.dtype, device=quant_bias.device)
        input_scale = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_scale')
        input_offset = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_offset')
        weight = (qweight, weight_scale, weight_offset, deq_scale, quant_bias, input_scale, input_offset)
        return weight


    def get_multi_weights_row(self, prefix: str, quantize: str, gqa_size=1, dim=1):
        if quantize in [QuantType.W8A8, QuantType.W8A8S]:
            qweight = self.get_sharded(f"{prefix}.weight", dim=dim, gqa_size=gqa_size)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = self.get_tensor(f"{prefix}.deq_scale")
            quant_bias = self.get_tensor(f"{prefix}.quant_bias")
            if self.process_group.rank() == 0:
                quant_bias = quant_bias
            else:
                quant_bias = torch.zeros_like(quant_bias, dtype=quant_bias.dtype, device=quant_bias.device)
            input_scale = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_scale')
            input_offset = self.get_per_tensor_sharded([prefix], dim=0, tensor_name='input_offset')
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset)
        elif quantize in [QuantType.W4A16, QuantType.W8A16, QuantType.W8A8_DYNAMIC]:
            weight_effective_head_size = gqa_size
            # The new version of int4 weight is packed by default.
            # Thus, when the attention head axis is partitioned, head size should be cut in half.
            if quantize == QuantType.W4A16 and self.quant_desc.get("group_size", 0) == 0 \
                and self.quant_desc.get("version", "0.0.0") != "0.0.0":
                weight_effective_head_size = (gqa_size + 1) // 2  # round up
            qweight = self.get_sharded(f"{prefix}.weight", dim=1, gqa_size=weight_effective_head_size)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_sharded(f"{prefix}.weight_scale", dim=dim, gqa_size=1)
            weight_offset = self.get_sharded(f"{prefix}.weight_offset", dim=dim, gqa_size=1)
            weight = (qweight, weight_scale, weight_offset)
        elif quantize == QuantType.W8A8SC:
            return self.get_w8a8sc_weight(prefix)
        elif quantize == QuantType.W8A8_PDMIX:
            return self.get_w8a8_pdmix_weight_row(prefix, gqa_size, dim)
        elif quantize == QuantType.W4A8_DYNAMIC:
            qweight = self.get_sharded(f"{prefix}.weight", dim=1, gqa_size=gqa_size)
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_sharded(f"{prefix}.weight_scale", dim=dim, gqa_size=1)
            weight_bias = self.get_sharded(f"{prefix}.scale_bias", dim=dim, gqa_size=1)
            if self.quant_desc.get("group_size") == 0:
                weight = (qweight, weight_scale, weight_bias)
            else:
                weight_scale_second = self.get_sharded(f"{prefix}.weight_scale_second", dim=dim, gqa_size=1)
                weight = (qweight, weight_scale, weight_scale_second, weight_bias) 
        elif quantize == QuantType.W16A16SC:
            qweight = self.get_tensor(f"{prefix}.weight")
            index = self.get_tensor(f"{prefix}.index")
            self.weight_dims = self.load_dims(self.process_group)
            dim_n = self.weight_dims[f"{prefix}.weight.outdim"]
            if self.weight_dims.get(f"{prefix}.bias.outdim"):
                quant_bias = self.get_tensor(f"{prefix}.bias").to(torch.float32)
            else:
                quant_bias = torch.zeros(dim_n, dtype=torch.float32)
            weight = (qweight, index, quant_bias)  
        else:
            weight = self.get_sharded(f"{prefix}.weight", dim=dim, gqa_size=gqa_size)
        return weight

    def get_replicated_weights(self, prefix: str, quantize: str):
        if quantize in [QuantType.W8A8, QuantType.W8A8S]:
            qweight = self.get_tensor(f"{prefix}.weight")
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            deq_scale = self.get_tensor(f"{prefix}.deq_scale")
            quant_bias = self.get_tensor(f"{prefix}.quant_bias")
            input_scale = self.get_tensor(f"{prefix}.input_scale")
            input_offset = self.get_tensor(f"{prefix}.input_offset")
            weight = (qweight, deq_scale, quant_bias, input_scale, input_offset)
        elif quantize in [QuantType.W4A16, QuantType.W8A16, QuantType.W8A8_DYNAMIC]:
            qweight = self.get_tensor(f"{prefix}.weight")
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_tensor(f"{prefix}.weight_scale")
            weight_offset = self.get_tensor(f"{prefix}.weight_offset")
            weight = (qweight, weight_scale, weight_offset)
        elif quantize == QuantType.W4A8_DYNAMIC:
            qweight = self.get_tensor(f"{prefix}.weight")
            if qweight.dtype in [torch.float16, torch.bfloat16]:
                return qweight
            weight_scale = self.get_tensor(f"{prefix}.weight_scale")
            weight_bias = self.get_tensor(f"{prefix}.bias", ignore_tensor_correction=True)
            weight = (qweight, weight_scale, weight_bias)
        elif quantize == QuantType.W8A8SC:
            return self.get_w8a8sc_weight(prefix)
        else:
            weight = self.get_tensor(f"{prefix}.weight")
        return weight

    def get_version(self):
        return self.version
    
    def get_nzcasted_weights(self, config, prefixes):
        quantize = getattr(config, "quantize", QuantType.FLOAT)
        if quantize == QuantType.W8A8_DYNAMIC:
            hidden_size = getattr(config, "hidden_size", 7168)
            AtlasGMMPermute = getattr(config, "AtlasGMMPermute", False)
            tensor_list = []
            is_down = "down" in prefixes[0]
            for prefix in prefixes:
                nz_tensor_shape = self._get_slice(f"{prefix}.weight").get_shape()
                dim = 1 if nz_tensor_shape[0] == hidden_size else 0
                nz_tensor = self.get_whole_tensor(f"{prefix}.weight", dim)
                if is_down:
                    weight_scale = self.get_sharded(f"{prefix}.weight_scale", dim=dim, gqa_size=1)
                else:
                    weight_scale = self.get_whole_tensor(f"{prefix}.weight_scale", dim)
                weight_offset = self.get_sharded(f"{prefix}.weight_offset", dim=dim, gqa_size=1)
                weight = [nz_tensor, weight_scale, weight_offset]
                tensor_list.append(weight)
            if is_down:
                tensor_list = tensor_list[0]
            else:
                weight = torch.cat((tensor_list[0][0], tensor_list[1][0]), dim=dim)
                weight_scale = torch.cat((tensor_list[0][1], tensor_list[1][1]), dim=0)
                weight_offset = torch.cat((tensor_list[0][2], tensor_list[1][2]), dim=0)

                tensor_list = [weight, weight_scale]
  
            tensor_list[0] = self._nz2nd(tensor_list[0])
            if not is_down and AtlasGMMPermute:
                tensor_list[0] = self._unpermute(tensor_list[0], -1)
                tensor_list[1] = self._unpermute(tensor_list[1], -2)

    
            qweight = self._slice_tensor(tensor_list[0], dim, is_down)
            if not is_down:
                weight_scale = self._slice_tensor(tensor_list[1], 0, is_down)
            if (not is_down):
                offset = tensor_list[0].shape[dim] // 2
                qweight = torch.cat((qweight, self._slice_tensor(tensor_list[0], \
                                    dim, is_down, offset)), dim=dim).transpose(-1, -2).contiguous()
                weight_scale = torch.cat((weight_scale, self._slice_tensor(tensor_list[1], 0, is_down, offset)), dim=0)

            return (qweight, weight_scale, weight_offset)
        else:
            raise ValueError("nz weight does not support loaded other than w8a8_dynamic")
    
    def _slice_tensor(self, tensor, dim, is_down=True, offset=0):
        world_size = self.process_group.size()
        rank = self.process_group.rank()
        block_size = tensor.shape[dim] // world_size
        if not is_down:
            block_size = block_size // 2
        start = rank * block_size + offset
        stop = (rank + 1) * block_size + offset
        if dim == 0:
            weight = tensor[start:stop]
        elif dim == 1:
            weight = tensor[:, start:stop]
        return weight

    def _get_handle(self, filename):
        if filename not in self._handles:
            f = safe_open(filename, framework="pytorch")
            self._handles[filename] = f
        return self._handles[filename]

    def _get_slice(self, tensor_name: str):
        filename, tensor_name = self.get_filename(tensor_name)
        f = self._get_handle(filename)
        slice_ = f.get_slice(tensor_name)
        return slice_

    def _get_gptq_params(self) -> Tuple[int, int]:
        try:
            bits = self.get_tensor("gptq_bits").item()
            groupsize = self.get_tensor("gptq_groupsize").item()
        except (SafetensorError, RuntimeError) as _:
            try:
                bits = self.gptq_bits
                groupsize = self.gptq_groupsize
            except Exception as err:
                raise AssertionError from err

        return bits, groupsize

    def _set_quant_params(self, model_id):
        filename = os.path.join(model_id, 'quant_model_description.json')
        if file_utils.is_path_exists(filename):
            with file_utils.safe_open(filename, 'r', check_link=False) as f:
                data = json.load(f)
            self.quant_desc = data
            return

        print_log(self.process_group.rank(), logger.warning,
                  "After 2026/06/30, the existing quantization weight will be degraded. " \
                  "To generate the latest version, please upgrade MindStudio ModelSlim.", True)

        try:
            if self.quantize in [QuantType.W8A8_PDMIX, QuantType.W8A8_DYNAMIC]:
                filename = os.path.join(model_id, f'quant_model_description_{self.quantize}.json')
                if not file_utils.is_path_exists(filename):
                    warning_msg = f"Quant description file of quant type {self.quantize} not found in model path," \
                        " try to use W8A8 instead."
                    print_log(self.process_group.rank(), logger.warning, warning_msg, True)
                    filename = os.path.join(model_id, f'quant_model_description_{QuantType.W8A8.value}.json')
            else:                
                filename = os.path.join(model_id, f'quant_model_description_{self.quantize}.json')
            with file_utils.safe_open(filename, 'r', check_link=False) as f:
                data = json.load(f)
            self.quant_desc = data
        except Exception as err:
            raise AssertionError from err
