#!/usr/bin/env python
# coding=utf-8
# Copyright (c) Huawei Technologies Co., Ltd. 2024-2025. All rights reserved.
# MindIE is licensed under Mulan PSL v2.
# You can use this software according to the terms and conditions of the Mulan PSL v2.
# You may obtain a copy of Mulan PSL v2 at:
#          http://license.coscl.org.cn/MulanPSL2
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
# See the Mulan PSL v2 for more details.

import argparse
import os
import re
from dataclasses import dataclass
from functools import wraps
from typing import Any, Callable, List, Optional, Tuple, Union, TYPE_CHECKING

import torch
import torch_npu
from transformers import PretrainedConfig, PreTrainedTokenizer, PreTrainedModel, AutoConfig, AutoTokenizer
from transformers.dynamic_module_utils import get_class_from_dynamic_module
from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions, SequenceClassifierOutput
from transformers.tokenization_utils_base import BatchEncoding

from atb_llm.models.base.model_utils import safe_from_pretrained


if TYPE_CHECKING:
    from optimum.onnxruntime import ORTModel
    from ais_bench.infer.interface import InferSession


TRUE = "true"
FALSE = "false"

INPUT_IDS = "input_ids"
ATTENTION_MASK = "attention_mask"
TOKEN_TYPE_IDS = "token_type_ids"
POSITION_IDS = "position_ids"
NONZERO_SEQ_LEN = "nonzero_seq_len"

CPU = "cpu"
GPU = "cuda"
NPU = "npu"


def check_import(exception: Optional[Exception]) -> Callable:
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            if exception:
                raise RuntimeError("unsupported model type") from exception
            return func(*args, **kwargs)
        return wrapper
    return decorator


def get_auto_model_cls(architectures: Union[str, List[str]]) -> str:
    architecture = architectures[0] if isinstance(architectures, list) else architectures
    match = re.search(r"([A-Z][a-z]+)$", architecture)
    if match:
        return match.group(0)
    else:
        raise ValueError(f"unsupported architecture: {architecture}, please check config.json.")


def get_model_from_pretrained(
        config: PretrainedConfig,
        cls_name: str,
        model_name_or_path: Union[str, os.PathLike]
) -> PreTrainedModel:
    model_type = config.model_type.replace("-", "_")
    modeling_dir = os.path.join(os.path.dirname(__file__), model_type)
    architecture = config.architectures[0] if isinstance(config.architectures, list) else config.architectures
    class_ref = f"{modeling_dir}--modeling_{model_type}.{architecture}"
    model_class = get_class_from_dynamic_module(class_ref, model_name_or_path)
    model_class.register_for_auto_class(cls_name)
    return model_class.from_pretrained(model_name_or_path, config=config)


def create_nonpadded_input_ids(
        input_ids: torch.Tensor,
        padding_idx: Optional[int] = None
) -> torch.Tensor:
    nonpadded_input_ids = input_ids.clone().view(-1)
    nonzero_indices = nonpadded_input_ids.ne(padding_idx).int().nonzero().squeeze()
    return nonpadded_input_ids[nonzero_indices].unsqueeze(0)


def create_nonpadded_position_ids(
        nonzero_seq_len: torch.Tensor,
        padding_idx: Optional[int] = None
) -> torch.Tensor:
    if padding_idx:
        padding_idx += 1
    return torch.cat([torch.arange(padding_idx, seq_len + padding_idx) for seq_len in nonzero_seq_len], dim=0)


def create_nonpadded_attention_mask(
        attention_mask: torch.Tensor,
        nonzero_seq_len: Optional[torch.Tensor] = None
) -> torch.Tensor:
    last_position_id = torch.cumsum(nonzero_seq_len, dim=0)
    padded_attention_mask = torch.zeros(
        (1, nonzero_seq_len.sum().item(), nonzero_seq_len.sum().item()),
        dtype=attention_mask.dtype,
        device=attention_mask.device
    )
    for seq_len, position_id in zip(nonzero_seq_len, last_position_id):
        padded_attention_mask[:, position_id - seq_len:position_id, position_id - seq_len:position_id] = 1.0
    return padded_attention_mask


class FloatModel:

    try:
        from transformers import AutoModel, AutoModelForSequenceClassification
    except ImportError as e:
        exception = e
    else:
        exception = None
        architectures_map = {
            "Model": {
                "cls": AutoModel,
                "output": BaseModelOutputWithPoolingAndCrossAttentions
            },
            "Classification": {
                "cls": AutoModelForSequenceClassification,
                "output": SequenceClassifierOutput
            }
        }

    @classmethod
    @check_import(exception)
    def get_model_ins(
            cls,
            config: PretrainedConfig,
            architecture: str,
            model_name_or_path: Union[str, os.PathLike],
            **kwargs: Any
    ) -> PreTrainedModel:
        device = kwargs.pop("device", torch.device(CPU))
        torch_dtype = kwargs.pop("torch_dtype", torch.float16)
        return get_model_from_pretrained(
            config,
            cls.architectures_map.get(architecture).get("cls").__name__,
            model_name_or_path
        ).to(torch_dtype).to(device).eval()

    @classmethod
    @check_import(exception)
    @torch.no_grad()
    def forward(
            cls,
            inputs: BatchEncoding,
            model: PreTrainedModel,
            architecture: str
    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, SequenceClassifierOutput]:
        return model(**inputs, return_dict=True)


class ONNXModel:

    try:
        from optimum.onnxruntime import ORTModelForCustomTasks, ORTModelForSequenceClassification
    except ImportError as e:
        exception = e
    else:
        exception = None
        architectures_map = {
            "Model": {
                "cls": ORTModelForCustomTasks,
                "output": BaseModelOutputWithPoolingAndCrossAttentions
            },
            "Classification": {
                "cls": ORTModelForSequenceClassification,
                "output": SequenceClassifierOutput
            }
        }

    @classmethod
    @check_import(exception)
    def get_model_ins(
            cls,
            config: PretrainedConfig,
            architecture: str,
            model_name_or_path: Union[str, os.PathLike],
            **kwargs: Any
    ) -> "ORTModel":
        device = kwargs.pop("device", torch.device(CPU))
        torch_dtype = kwargs.pop("torch_dtype", torch.float16)
        trust_remote_code = kwargs.pop("trust_remote_code", False)
        return safe_from_pretrained(
            cls.architectures_map.get(architecture).get("cls"),
            model_name_or_path,
            torch_dtype=torch_dtype,
            trust_remote_code=trust_remote_code
        ).to(device)

    @classmethod
    @check_import(exception)
    @torch.inference_mode()
    def forward(
            cls,
            inputs: BatchEncoding,
            model: "ORTModel",
            architecture: str
    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, SequenceClassifierOutput]:
        return model(**inputs)


class OMModel:

    try:
        from ais_bench.infer.interface import InferSession
    except ImportError as e:
        exception = e
    else:
        exception = None
        architectures_map = {
            "Model": {
                "cls": InferSession,
                "output": BaseModelOutputWithPoolingAndCrossAttentions
            },
            "Classification": {
                "cls": InferSession,
                "output": SequenceClassifierOutput
            }
        }

    @classmethod
    @check_import(exception)
    def get_model_ins(
            cls,
            config: PretrainedConfig,
            architecture: str,
            model_name_or_path: Union[str, os.PathLike],
            **kwargs: Any
    ) -> "InferSession":
        device = kwargs.pop("device", torch.device(CPU))
        om_model_file = next(iter([file for file in os.listdir(model_name_or_path) if file.endswith(".om")]), "")
        return cls.architectures_map.get(architecture).get("cls")(
            device.index,
            os.path.join(model_name_or_path, om_model_file)
        )

    @classmethod
    @check_import(exception)
    def forward(
            cls,
            inputs: BatchEncoding,
            model: "InferSession",
            architecture: str
    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, SequenceClassifierOutput]:
        input_ids = inputs.get("input_ids")
        attention_mask = inputs.get("attention_mask")
        feeds = [input_ids, attention_mask]

        if "token_type_ids" in inputs:
            feeds.append(inputs.get("token_type_ids"))
        if "position_ids" in inputs:
            feeds.append(inputs.get("position_ids"))

        dtype_size = 4
        hidden_size = 1024
        size = dtype_size * hidden_size * input_ids.numel()

        model_output = model.infer(feeds=feeds, mode="dymshape", custom_sizes=size)
        model_output = torch.from_numpy(model_output[0])

        return cls.architectures_map.get(architecture).get("output")(model_output)


class ModelFactory:

    model_factory_map = {
        "float": FloatModel,
        "onnx": ONNXModel,
        "om": OMModel
    }

    @classmethod
    def get_model_ins(
            cls,
            config: PretrainedConfig,
            architecture: str,
            model_name_or_path: Union[str, os.PathLike],
            model_type: str,
            **kwargs: Any
    ) -> Any:
        if model_type in cls.model_factory_map:
            return cls.model_factory_map.get(model_type).get_model_ins(
                config,
                architecture,
                model_name_or_path,
                **kwargs
            )
        raise ValueError(f"unsupported model type: {model_type}.")

    @classmethod
    def forward(
            cls,
            inputs: BatchEncoding,
            model: Union[PreTrainedModel, "ORTModel", "InferSession"],
            architecture: str,
            model_type: str
    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, SequenceClassifierOutput]:
        if model_type in cls.model_factory_map:
            return cls.model_factory_map.get(model_type).forward(
                inputs,
                model,
                architecture
            )
        raise ValueError(f"unsupported model type: {model_type}.")


@dataclass
class ModelCls:
    config: PretrainedConfig
    tokenizer: PreTrainedTokenizer
    model: Union[PreTrainedModel, "ORTModel", "InferSession"]


@dataclass
class TokenizerParams:
    padding: Union[str, bool] = True
    truncation: Union[str, bool] = True
    return_tensors: str = "pt"
    max_length: Union[str, int] = 512


class ModelRunner:
    def __init__(
            self,
            model_name_or_path: Union[str, os.PathLike],
            trust_remote_code: bool,
            torch_dtype: torch.dtype,
            device: torch.device,
            model_type: str,
    ) -> None:
        self.model_name_or_path = model_name_or_path
        self.trust_remote_code = trust_remote_code
        self.torch_dtype = torch_dtype
        self.device = device
        self.model_type = model_type

        self.init_device()

        self.model_cls = self.get_model_cls()
        self.config = self.model_cls.config
        self.tokenizer = self.model_cls.tokenizer
        self.model = self.model_cls.model

        self.architecture = get_auto_model_cls(self.config.architectures)

    def init_device(self):
        if self.model_type == "om":
            self.device = torch.device(CPU)
            return
        if self.device.type == GPU:
            torch.cuda.set_device(self.device.index)
        elif self.device.type == NPU:
            torch_npu.npu.set_device(self.device.index)
            torch_npu.npu.set_compile_mode(jit_compile=False)

    def get_model_cls(self) -> ModelCls:
        config = safe_from_pretrained(
            AutoConfig,
            self.model_name_or_path,
            trust_remote_code=self.trust_remote_code
        )
        tokenizer = safe_from_pretrained(
            AutoTokenizer,
            self.model_name_or_path,
            trust_remote_code=self.trust_remote_code
        )
        model = ModelFactory.get_model_ins(
            config,
            get_auto_model_cls(config.architectures),
            self.model_name_or_path,
            self.model_type,
            device=self.device,
            torch_dtype=self.torch_dtype,
            trust_remote_code=self.trust_remote_code
        )

        return ModelCls(config, tokenizer, model)

    def create_encoded_inputs(
            self,
            encoded_inputs: BatchEncoding,
            nonpadding: bool = False
    ) -> BatchEncoding:
        if not nonpadding:
            encoded_inputs.update({NONZERO_SEQ_LEN: None})
            return BatchEncoding(encoded_inputs.data)

        nonzero_seq_len = encoded_inputs[INPUT_IDS].ne(self.config.pad_token_id).count_nonzero(-1)
        position_pad_tokens = 2 if self.tokenizer.__class__.__name__.startswith("XLMRoberta") else 0
        max_position_embeddings = self.config.max_position_embeddings - position_pad_tokens

        if nonzero_seq_len.sum().item() > max_position_embeddings:
            encoded_inputs.update({NONZERO_SEQ_LEN: None})
            return BatchEncoding(encoded_inputs.data)

        input_ids = create_nonpadded_input_ids(encoded_inputs[INPUT_IDS], self.config.pad_token_id)
        attention_mask = create_nonpadded_attention_mask(encoded_inputs[ATTENTION_MASK], nonzero_seq_len)
        token_type_ids = torch.zeros(input_ids.size(), dtype=torch.long, device=self.device)
        position_ids = create_nonpadded_position_ids(nonzero_seq_len, self.config.pad_token_id)
        encoded_inputs.update({
            INPUT_IDS: input_ids,
            ATTENTION_MASK: attention_mask,
            TOKEN_TYPE_IDS: token_type_ids,
            POSITION_IDS: position_ids,
            NONZERO_SEQ_LEN: nonzero_seq_len
        })
        return BatchEncoding(encoded_inputs.data)

    def generate_inputs(
            self,
            model_input_names: List[str],
            vocab_size: int,
            input_shape: Tuple[int, int],
            nonpadding: bool = False
    ) -> BatchEncoding:
        batch_size, seq_len = input_shape
        inputs = {}
        if INPUT_IDS in model_input_names:
            inputs[INPUT_IDS] = torch.randint(0, vocab_size, input_shape, dtype=torch.int64, device=self.device)
        if TOKEN_TYPE_IDS in model_input_names:
            inputs[TOKEN_TYPE_IDS] = torch.zeros(input_shape, dtype=torch.int64, device=self.device)
        if ATTENTION_MASK in model_input_names:
            inputs[ATTENTION_MASK] = torch.randint(0, 2, input_shape, dtype=torch.int64, device=self.device)
        if POSITION_IDS in model_input_names:
            inputs[POSITION_IDS] = torch.arange(0, seq_len, dtype=torch.int64, device=self.device)
        return self.create_encoded_inputs(BatchEncoding(inputs), nonpadding=nonpadding)

    def tokenize(
            self,
            sentences: Union[str, List[str], List[List[str]]],
            tokenizer_params: TokenizerParams,
            nonpadding: bool = False
    ) -> BatchEncoding:
        if isinstance(sentences, str):
            sentences = [sentences]
        encoded_inputs = self.tokenizer(
            sentences,
            padding=tokenizer_params.padding,
            truncation=tokenizer_params.truncation,
            return_tensors=tokenizer_params.return_tensors,
            max_length=tokenizer_params.max_length
        ).to(self.device)
        return self.create_encoded_inputs(encoded_inputs, nonpadding=nonpadding)

    def forward(
            self,
            inputs: BatchEncoding
    ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, SequenceClassifierOutput]:
        return ModelFactory.forward(
            inputs,
            self.model,
            self.architecture,
            self.model_type
        )

    def embed(
            self,
            inputs: BatchEncoding
    ) -> torch.Tensor:
        results = ModelFactory.forward(
            inputs,
            self.model,
            self.architecture,
            self.model_type
        )[0][:, 0]
        results = torch.nn.functional.normalize(results, p=2, dim=1).cpu()
        return results

    def rerank(
            self,
            inputs: BatchEncoding
    ) -> torch.Tensor:
        results = ModelFactory.forward(
            inputs,
            self.model,
            self.architecture,
            self.model_type
        ).logits.view(-1, ).float()
        results = results.cpu()
        return results


class Arguments:

    parser = argparse.ArgumentParser()

    def __init__(self):
        self.set_common_args()

    @classmethod
    def set_runner_args(cls):
        parser = cls().parser
        parser.add_argument(
            "request",
            type=str,
            choices=["embed", "rerank"]
        )
        parser.add_argument(
            "--texts",
            type=str,
            nargs='+',
            default=["样例数据-1", "样例数据-2"]
        )
        parser.add_argument(
            "--max_batch_size",
            type=int,
            default=1
        )
        return parser.parse_args()

    @classmethod
    def set_tester_args(cls):
        parser = cls().parser
        parser.add_argument(
            "task",
            type=str,
            choices=["performance", "retrieval", "reranking"]
        )
        parser.add_argument(
            "--dataset_path",
            help="dataset path"
        )
        parser.add_argument(
            "--batch_size",
            type=int,
            default=20
        )
        parser.add_argument(
            "--loop",
            type=int,
            default=100
        )
        parser.add_argument(
            "--outputs",
            type=str,
            default="results"
        )
        return parser.parse_args()

    def set_common_args(self):
        self.parser.add_argument(
            "--model_name_or_path",
            help="model and tokenizer path"
        )
        self.parser.add_argument(
            "--trust_remote_code",
            action="store_true"
        )
        self.parser.add_argument(
            "--nonpadding",
            action="store_true"
        )
        self.parser.add_argument(
            "--model_type",
            type=str,
            choices=["float", "onnx", "om"],
            default="float"
        )
        self.parser.add_argument(
            "--torch_dtype",
            type=str,
            default="float16"
        )
        self.parser.add_argument(
            "--device_type",
            type=str,
            choices=[CPU, GPU, NPU],
            default=CPU
        )
        self.parser.add_argument(
            "--device_id",
            type=int,
            default=0
        )
        self.parser.add_argument(
            "--padding",
            type=lambda value: value.lower() == TRUE if value.lower() in [TRUE, FALSE] else value,
            default=True
        )
        self.parser.add_argument(
            "--truncation",
            type=lambda value: value.lower() == TRUE if value.lower() in [TRUE, FALSE] else value,
            default=True
        )
        self.parser.add_argument(
            "--return_tensors",
            type=str,
            choices=["pt", "np"],
            default="pt"
        )
        self.parser.add_argument(
            "--max_seq_len",
            type=int,
        )
