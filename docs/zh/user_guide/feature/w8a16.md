# W8A16

## 简介

此量化方式对激活值不做量化，仅将权重量化为8 bit。使用per Channel量化。

> [!NOTE]说明 
>-  仅Atlas 800I A2 推理服务器支持此量化方式。
>-  仅支持LLaMA3-70B。
>-  仅支持和Anti-Outlier离群值处理、KV Cache int8量化配合使用。

量化后权重目录结构：

```
├─ config.json
├─ quant_model_weight_w8a16.safetensors
├─ quant_model_description.json
├─ tokenizer_config.json
├─ tokenizer.json
└─ tokenizer.model
```

-  量化输出包含：权重文件quant\_model\_weight\_w8a16.safetensors和权重描述文件quant\_model\_description.json。
-  目录中的其余文件为推理时所需的配置文件，不同模型略有差异。

以下展示了量化后权重描述文件quant\_model\_description.json中的部分内容：

```json
{
  "model_quant_type": "W8A16",
  "model.embed_tokens.weight": "FLOAT",
  "model.layers.0.self_attn.q_proj.weight": "W8A16",
  "model.layers.0.self_attn.q_proj.weight_scale": "W8A16",
  "model.layers.0.self_attn.q_proj.weight_offset": "W8A16",
}
```

量化后的MatMul权重新增weight\_scale和weight\_offset，用于对MatMul的计算结果进行反量化。

**图 1**  量化权重推理时流程<a name="fig132131518185315"></a>  
![](../../figures/w8a16.png "量化权重推理时流程-1")

此量化方式支持量化float16或bfloat16类型的原始权重。

**表 1**  float16权重量化后dtype及shape信息（假设原始权重的shape为\[n, k\]）

|Tensor信息|weight|weight_scale|weight_offset|bias|
|--|--|--|--|--|
|dtype|int8|float32|float32|float16|
|shape|[n, k]|[n, 1]|[n, 1]|[n]|


**表 2**  bfloat16权重量化后dtype及shape信息（假设原始权重的shape为\[n, k\]）

|Tensor信息|weight|weight_scale|weight_offset|bias|
|--|--|--|--|--|
|dtype|int8|float32|float32|float32|
|shape|[n, k]|[n, 1]|[n, 1]|[n]|


> [!NOTE]说明 
> 仅当浮点权重存在bias场景时，量化权重才会有bias。

## 生成权重

以LLaMA3-70B为例，您可以使用以下指令生成W8A16量化权重。

```bash
cd ${ATB_SPEED_HOME_PATH}
get_llama3_70b_disable_name() {
    local num_layer=$1
    local disable_names=""
    for ((i=0; i<5; i++)); do
    {
        disable_names="$disable_names model.layers.$i.mlp.down_proj"
        disable_names="$disable_names model.layers.$i.self_attn.q_proj"
        disable_names="$disable_names model.layers.$i.self_attn.k_proj"
        disable_names="$disable_names model.layers.$i.self_attn.v_proj"
        disable_names="$disable_names model.layers.$i.self_attn.o_proj"
        disable_names="$disable_names model.layers.$i.mlp.gate_proj"
        disable_names="$disable_names model.layers.$i.mlp.up_proj"
    }
    done
    disable_names="$disable_names lm_head"
    echo "$disable_names"
}
disable_names=$(get_llama3_70b_disable_name 80)
python examples/models/llama3/convert_quant_weights.py --model_path {浮点权重路径} --save_directory {W8A8量化权重路径} --calib_file $ATB_SPEED_HOME_PATH/examples/convert/model_slim/boolq.jsonl --disable_names $disable_names --device_type npu --a_bit 16 --w_sym False --mm_tensor False --anti_method m3 --act_method 3 --tokenizer_args {\"padding_side\":\"left\",\"pad_token\":\"<unk>\"}
```

-  以上指令包含生成LLaMA3-70B W8A16权重的最优参数配置，不同模型的参数配置不同，请参考模型Readme文件。
-  W8A16量化时无需校准数据集，故calib\_file传入空字符串即可。

## 执行推理

以LLaMA3-70B为例，您可以使用以下指令执行对话测试，推理内容为"What's deep learning?"。

```bash
cd ${ATB_SPEED_HOME_PATH}
bash examples/models/llama/run_pa.sh {W8A16量化权重路径}
```

