# W8A8

## 简介

此量化方式对权重和激活值均进行量化，将高位浮点数转为8 bit，减少模型权重的体积。使用int8格式的数据进行计算，可以减少MatMul算子计算量，以提升推理性能。

> [!NOTE]说明 
>-  仅支持Qwen3-235B-A22B，Qwen3-30B-A3B，DeepSeek-R1,DeepSeek-V2-Chat,DeepSeek-V3,Mixtral-8x7B-Instruct-V0.1,Baichuan2-7B,Baichuan2-13B,ChatGLM3-6B,GLM4-9B,DeepSeek-R1-Distill-Llama-8B,DeepSeek-R1-Distill-Llama-70B,DeepSeek-R1-Distill-Qwen-1.5B，DeepSeek-R1-Distill-Qwen-7B，DeepSeek-R1-Distill-Qwen-14B，DeepSeek-R1-Distill-Qwen-32B，LLaMA3.1-8B，LLaMA3.1-70B，Qwen3-14B，Qwen3-32B，Qwen2-7B，Qwen2-72B，Qwen2.5-7B，Qwen2.5-14B，Qwen2.5-32B和Qwen2.5-72B。
>-  仅支持和PD分离、SplitFuse、Prefix Cache、并行解码、Function Call、长序列以及多机推理特性配合使用。
>-  仅支持和Attention、KV Cache量化配合使用。

量化后权重目录结构：

```
├─ config.json
├─ quant_model_weight_w8a8.safetensors
├─ quant_model_description.json
├─ tokenizer_config.json
├─ tokenizer.json
└─ tokenizer.model
```

-   量化输出包含：权重文件quant\_model\_weight\_w8a8.safetensors和权重描述文件quant\_model\_description.json。
-   目录中的其余文件为推理时所需的配置文件，不同模型略有差异。

以下展示了量化后权重描述文件quant\_model\_description.json中的部分内容：

```
{
  "model_quant_type": "W8A8",
  "model.embed_tokens.weight": "FLOAT",
  "model.layers.0.self_attn.q_proj.weight": "W8A8",
  "model.layers.0.self_attn.q_proj.input_scale": "W8A8",
  "model.layers.0.self_attn.q_proj.input_offset": "W8A8",
  "model.layers.0.self_attn.q_proj.quant_bias": "W8A8",
  "model.layers.0.self_attn.q_proj.deq_scale": "W8A8"
}
```

量化后的MatMul权重新增input\_scale、input\_offset、quant\_bias和deq\_scale。其中input\_scale和input\_offset用于对激活值进行量化。MatMul使用量化后的激活值和量化权重进行计算。quant\_bias和deq\_scale用于对MatMul的计算结果进行反量化。

**图 1**  量化权重推理时流程<a name="fig82011323195212"></a>  
![](../../figures/w8a8.png "量化权重推理时流程")

此量化方式支持量化float16或bfloat16类型的原始权重。

**表 1**  float16权重量化后dtype及shape信息（假设原始权重的shape为\[n, k\]）

|Tensor信息|weight|input_scale|input_offset|quant_bias|deq_scale|
|--|--|--|--|--|--|
|dtype|int8|float16|float16|int32|int64|
|shape|[n, k]|[1]|[1]|[n]|[n]|


**表 2**  bfloat16权重量化后dtype及shape信息（假设原始权重的shape为\[n, k\]）

|Tensor信息|weight|input_scale|input_offset|quant_bias|deq_scale|
|--|--|--|--|--|--|
|dtype|int8|bfloat16|bfloat16|int32|float32|
|shape|[n, k]|[1]|[1]|[n]|[n]|


## 生成权重

以LLaMA3.1-8B为例，您可以使用以下指令生成W8A8量化权重。

```
cd ${ATB_SPEED_HOME_PATH}
python examples/models/llama3/convert_quant_weights.py --model_path {浮点权重路径} --save_directory {W8A8量化权重路径} --w_bit 8 --a_bit 8 --disable_level L0 --device_type cpu --anti_method m1 --act_method 1 --calib_file ${llm_path}/examples/convert/model_slim/boolq.jsonl
```

以上指令展示了生成LLaMA3.1-8B W8A8权重的最优参数配置，不同模型的参数配置不同，请参考模型Readme文件。

## 执行推理

以LLaMA3.1-8B为例，您可以使用以下指令执行对话测试，推理内容为"What's deep learning?"，最长输出20个token。

```
cd ${ATB_SPEED_HOME_PATH}
bash examples/models/llama3/run_pa.sh {W8A8量化权重路径} 20
```

