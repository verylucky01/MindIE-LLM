#!/usr/bin/env python
# coding=utf-8
# Copyright (c) Huawei Technologies Co., Ltd. 2025. All rights reserved.
# MindIE is licensed under Mulan PSL v2.
# You can use this software according to the terms and conditions of the Mulan PSL v2.
# You may obtain a copy of Mulan PSL v2 at:
#          http://license.coscl.org.cn/MulanPSL2
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
# See the Mulan PSL v2 for more details.

import unittest
import sys
from unittest.mock import MagicMock, patch
from dataclasses import dataclass
import logging
from ddt import ddt
import torch
import numpy as np

from atb_llm.models import InferenceMode
from mindie_llm.text_generator.utils.model_input import ModelInput


MAX_POSITION_EMBEDDINGS = "max_position_embeddings"


@dataclass
class MockConfig:
    max_position_embeddings: int = 1024


class MockModelRunner:
    def __init__(self):
        self.config = MockConfig()
        self.config_dict = {MAX_POSITION_EMBEDDINGS: 0}
        self.tokenizer = None
        self.device = torch.device("cpu")
        self.rank = 0
        self.mapping = MagicMock()
        self.mapping.attn_dp.group_size = 8
        self.mapping.attn_inner_sp.group_size = 1
        self.process_group = MagicMock()
        self.process_group.size = MagicMock(return_value=8)
        self.kv_cache_dtype = torch.int8
        self.num_layers = 10
        self.num_kv_heads = 4
        self.head_size = 128
        self.k_head_size = 576
        self.v_head_size = 1
        self.model = MagicMock()
        self.model.max_position_embeddings = 1024
        self.soc_info = None
        self.adapter_manager = None
        self.max_position_embeddings = 1024
        self.enable_nz = False
        self.kvcache_quant_layers = []


@ddt
class TestATBModelWrapper(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        sys.modules['_libatb_torch'] = MagicMock()

    @classmethod
    def tearDownClass(cls):
        del sys.modules['_libatb_torch']

    @patch('mindie_llm.modeling.model_wrapper.atb.atb_model_wrapper.ModelRunner', return_value=MockModelRunner())
    def setUp(self, mock_model_runner):
        from mindie_llm.modeling.model_wrapper.atb.atb_model_wrapper import ATBModelWrapper

        self.additional_kwargs = {
            'backend_bin_path': '/usr/local/Ascend/mindie/2.0.RC1/mindie-llm/bin/',
            'backend_log_file': '/usr/local/Ascend/mindie/2.0.RC1/mindie-service/logs/mindie-server.log',
            'backend_modelInstance_id': '0',
            'backend_type': 'atb', 'block_size': 128, 'cpu_mem': 5, 'deploy_type': 'INTER_PROCESS',
            'dp': 8, 'executor_type': 'LLM_EXECUTOR_PYTHON', 'globalRankIds': '', 'globalWorldSize': '0',
            'interNodeKmcKsfMaster': 'tools/pmt/master/ksfa', 'interNodeKmcKsfStandby': 'tools/pmt/standby/ksfb',
            'interNodeTLSEnabled': '1', 'interNodeTlsCaFiles': 'ca.pem,', 'interNodeTlsCaPath': 'security/grpc/ca/',
            'interNodeTlsCert': 'security/grpc/certs/server.pem', 'interNodeTlsCrlFiles': 'server_crl.pem,',
            'interNodeTlsCrlPath': 'security/grpc/certs/', 'interNodeTlsPk': 'security/grpc/keys/server.key.pem',
            'interNodeTlsPkPwd': 'security/grpc/pass/mindie_server_key_pwd.txt', 'isMaster': '0', 'localIP': '',
            'log_error': '1', 'log_file_num': '20', 'log_file_size': '20', 'log_info': '1',
            'log_verbose': '0', 'log_warning': '1', 'masterIP': '', 'max_input_len': '2048', 'max_iter_times': '512',
            'max_prefill_tokens': '8192', 'max_seq_len': 2560, 'model_instance_number': '1',
            'model_instance_type': 'Standard', 'model_name': 'deepseek', 'moe_tp': 8, 'multiNodesInferEnabled': '0',
            'multiNodesInferPort': '1120', 'npu_device_ids': '0,1,2,3,4,5,6,7', 'npu_mem': -1, 'slaveIPs': '',
            'speculation_gamma': '0', 'tp': 1, 'trust_remote_code': '0', 'inference_mode': InferenceMode.REGRESSION,
            'plugin_params': {'plugin_type': None}
        }
        self.mock_model_runner = mock_model_runner
        self.mock_model_runner.return_value.load_weights = MagicMock()
        self.model_wrapper = ATBModelWrapper(
            rank=0, local_rank=0, world_size=8, npu_device_id=0,
            model_id="fake_model_id",
            **self.additional_kwargs
        )
        self.model_wrapper_edge = ATBModelWrapper(
            rank=0, local_rank=0, world_size=8, npu_device_id=0,
            model_id="fake_model_id", soc_version=240,
            **self.additional_kwargs
        )

    def test_forward(self):
        logits = torch.tensor([0, 1])
        self.mock_model_runner.return_value.forward = MagicMock(return_value=logits)
        input_ids = np.array([100000, 26503, 335, 279, 33396, 37576, 1593, 429, 39166])
        position_ids = np.array([0, 1, 2, 3, 4, 5, 60, 0, 1])
        block_tables = np.array([[0, 1], [0, 0]])
        slots = np.array([0, 1, 2, 3, 4, 5, 60, 0, 1])
        input_lengths = np.array([6, 2])
        model_input = ModelInput(
            input_ids, position_ids, block_tables, slots, input_lengths, max_seq_len=20,
            prefill_head_indices=None,
            is_prefill=True, query_length=None)
        dp_additional_args = {
            "token_size_per_dp_group": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "shard_effective_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "token_index_with_padding": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "skip_padding_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
        }

        res = self.model_wrapper.forward(model_input, **dp_additional_args)
        self.assertTrue(torch.equal(res, logits))
    
    def test_forward_sub_model_input(self):
        logits = torch.tensor([0, 1])
        self.mock_model_runner.return_value.forward = MagicMock(return_value=logits)
        input_ids = np.array([100000, 26503, 335, 279, 33396, 37576, 1593, 429, 39166])
        position_ids = np.array([0, 1, 2, 3, 4, 5, 60, 0, 1])
        block_tables = np.array([[0, 1], [0, 0]])
        slots = np.array([0, 1, 2, 3, 4, 5, 60, 0, 1])
        lm_head = np.arange(len(input_ids))
        input_lengths = np.array([6, 2])
        model_input = ModelInput(
            input_ids, position_ids, block_tables, slots, input_lengths, max_seq_len=20,
            prefill_head_indices=None,
            is_prefill=True, query_length=None)
        sub_model_inputs = ModelInput(
            input_ids, position_ids, block_tables, slots, input_lengths, max_seq_len=20,
            prefill_head_indices=lm_head,
            is_prefill=False, query_length=None)
        dp_additional_args = {
            "token_size_per_dp_group": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "shard_effective_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "token_index_with_padding": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "skip_padding_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "sub_model_inputs": sub_model_inputs
        }

        res = self.model_wrapper.forward(model_input, **dp_additional_args)
        self.assertTrue(torch.equal(res, logits))

    def test_forward_edge(self):
        logits = torch.tensor([0, 1])
        self.mock_model_runner.return_value.forward_tensor = MagicMock(return_value=logits)
        input_ids = np.array([100000, 26503, 335, 279, 33396, 37576, 1593, 429, 39166])
        position_ids = np.array([0, 1, 2, 3, 4, 5, 60, 0, 1])
        block_tables = np.array([[0, 1], [0, 0]])
        slots = np.array([0, 1, 2, 3, 4, 5, 60, 0, 1])
        input_lengths = np.array([6, 2])
        model_input = ModelInput(
            input_ids, position_ids, block_tables, slots, input_lengths, max_seq_len=20,
            prefill_head_indices=None,
            is_prefill=True, query_length=None)
        dp_additional_args = {
            "token_size_per_dp_group": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "shard_effective_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "token_index_with_padding": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "skip_padding_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
        }
        try:
            res = self.model_wrapper_edge.forward(model_input, **dp_additional_args)
            self.assertEqual(int(res[0].item()), int(logits[0].item()))
        except Exception as e:
            logging.exception(e)
    
    def test_dap_forward(self):
        logits = torch.tensor([0, 1])
        self.mock_model_runner.return_value.dap_forward = MagicMock(return_value=logits)
        # dap0
        input_ids0 = np.array([100000, 26503, 335, 279, 33396, 37576, 1593, 429,
            39166, 4403, 643, 895, 1377, 4712, 8254, 285, 33396, 433, 9017, 418, 839, 3430, 276, 2622,
            742, 2616, 11010, 15821, 11, 5802, 1094, 12191, 536, 441, 463, 276, 2622, 254, 11010, 3675,
            9880, 4712, 13, 685, 207, 17, 15, 15, 4, 11, 33396, 37576, 6972, 363, 18, 13,
            22, 19, 17, 10532, 881, 254, 2616, 39696, 13, 79073, 280, 33396, 37576, 2622, 881, 9798,
            12178, 11, 285, 418, 4117, 19336, 327, 9798, 12178, 7462, 2065, 20234, 13, 3159, 11, 657,
            418, 25541, 473, 254, 41198, 266, 12178, 47570, 13, 185, 23853, 25, 317, 11010, 9880, 4712,
            254, 1246, 372, 3613, 5424, 30, 185, 32349, 25, 100000, 26503, 335, 279, 33396, 37576, 1593,
            429, 39166, 4403, 643, 895, 1377, 4712, 8254, 285, 33396, 433, 9017, 418, 839, 3430, 276,
            2622, 742, 2616, 11010, 15821, 11, 5802, 1094, 12191, 536, 441, 463, 276, 2622, 254, 11010,
            3675, 9880, 4712, 13, 685, 207, 17, 15, 15, 24, 11, 33396, 37576, 6972, 363, 18,
            13, 22, 19, 17, 10532, 881, 254, 2616, 39696, 13, 79073, 280, 33396, 37576, 2622, 881,
            9798, 12178, 11, 285, 418, 4117, 19336, 327, 9798, 12178, 7462, 2065, 20234, 13, 3159, 11,
            657, 418, 25541, 473, 254, 41198, 266, 12178, 47570, 13, 185, 23853, 25, 317, 11010, 9880,
            4712, 254, 1246, 372, 3613, 5424, 30, 185, 32349, 25])
        position_ids0 = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
            13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
            39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,
            65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,
            91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103,
            104, 105, 106, 107, 108, 109, 110, 111, 112, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,
            43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,
            69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94,
            95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112])
        block_tables0 = np.array([[0, 1], [0, 1]])
        slots0 = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
            13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
            26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,
            39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
            52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64,
            65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
            78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,
            91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103,
            104, 105, 106, 107, 108, 109, 110, 111, 112, 0, 1, 2, 3,
            4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
            30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42,
            43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,
            56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,
            69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81,
            82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94,
            95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107,
            108, 109, 110, 111, 112])
        input_lengths0 = np.array([113, 113])
        max_seq_len0 = 113
        prefill_head_indices0 = np.array([112, 225])
        is_prefill = True
        dp_rank_ids0 = np.array([1, 1, 0, 0])
        query_length = None
        adapter_ids = None
        model_input0 = ModelInput(
            input_ids0, position_ids0, block_tables0, slots0, input_lengths0, max_seq_len0,
            prefill_head_indices0, is_prefill, query_length, adapter_ids, dp_rank_ids0)
        
        # dap1
        input_ids1 = np.array([100000, 2640, 6, 82, 4399, 4526, 30, 100000, 2819, 418, 340, 30])
        position_ids1 = np.array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4])
        block_tables1 = np.array([[0, 0], [0, 0]])
        slots1 = np.array([0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4])
        input_lengths1 = np.array([7, 5])
        max_seq_len1 = 7
        prefill_head_indices1 = np.array([6, 11])
        is_prefill = True
        dp_rank_ids1 = np.array([1, 1, 0, 0])
        model_input1 = ModelInput(
            input_ids1, position_ids1, block_tables1, slots1, input_lengths1, max_seq_len1,
            prefill_head_indices1, is_prefill, query_length, adapter_ids, dp_rank_ids1)

        dap_model_inputs = [model_input0]
        dap_model_inputs.append(model_input1)
        dep_inputs = [0, 1, 2]
        dp_additional_args0 = {
            "token_size_per_dp_group": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "shard_effective_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "token_index_with_padding": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "skip_padding_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "max_dp_batch_size": 2,
            "dep_inputs": dep_inputs
        }
        dp_additional_args1 = {
            "token_size_per_dp_group": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "shard_effective_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "token_index_with_padding": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "skip_padding_token_indices": np.array([0, 1, 2, 3, 4, 5, 60, 0, 1]),
            "max_dp_batch_size": 2,
            "dep_inputs": dep_inputs
        }
        dap_kwargs = [dp_additional_args0, dp_additional_args1]

        res = self.model_wrapper.dap_forward(dap_model_inputs, dap_kwargs, None)
        self.assertTrue(torch.equal(res, logits))


if __name__ == "__main__":
    unittest.main()
