# Copyright (c) Huawei Technologies Co., Ltd. 2024-2025. All rights reserved.
# MindIE is licensed under Mulan PSL v2.
# You can use this software according to the terms and conditions of the Mulan PSL v2.
# You may obtain a copy of Mulan PSL v2 at:
#          http://license.coscl.org.cn/MulanPSL2
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
# See the Mulan PSL v2 for more details.
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Union
import numpy as np

from ..plugins import InferenceMode
from ...modeling.backend_type import BackendType
from ...utils.log.error_code import ErrorCode
from ...utils.log.logging import logger

DEFAULT_SAMPLING_PARAMS = {
    "repetition_penalty": 1.0,
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0,
    "temperature": 1.0,
    "top_k": 0,
    "top_p": 1.0,
    "do_sample": False,
    "top_logprobs": 0,
}


class HandlingBackend(str, Enum):
    ATB = "atb"
    CPU = "cpu"
    PTA = "pta"
    MS = "ms"


@dataclass
class ModelConfig:
    """A data class of model configurations.

    The `ModelConfig` data class contains parameters required by the framework, for model inference, and for
    post-processing.

    Args:
        model_id: Model weight path.
        block_size: The number of slots a memory block consists of.
        cpu_mem: The amount of memory used as swap memory on the CPU when NPU memory is insufficient, in gigabytes (GB).
        npu_mem: The amount of memory used for kv cache on the NPU, in gigabytes (GB).
        local_rank: The rank of the current process on the local machine.
        npu_device_id: The device ID of the NPU used by the current process.
        rank: The rank of the current process within the entire cluster. In a single-node deployment scenario, it is
            equal to `local_rank`.
        world_size: The total number of processes in the cluster.
        backend_type: Backend type, with two options: 'atb' or 'ms'. This parameter must be consistent with the
            `MINDIE_LLM_FRAMEWORK_BACKEND` environment variable.
        max_input_len: The maximum length limit for the input sequence.
        max_iter_times: The maximum number of iterations, which can also be understood as the maximum length limit for
            the output generated by the inference framework.
        max_prefill_tokens: The maximum limit for the total length of all inputs in a batch during the prefilling phase.
        max_seq_len: The maximum number of tokens that the model can accept, including both input and output length.
            This parameter is used for stopping criteria during inference and does not affect model loading.
        ignore_eos: Whether to ignore EOS (End-of-Sequence) tokens during the stopping criteria phase of inference.
        model_role: The role of the current process in a PD (Prefilling-Decoding) separate deployment scenario, which
            can be 'prefill' or 'decoder'. The default is 'standard', meaning no PD separate deployment.
        num_threads: The number of threads used when sampling on the CPU.
        tokenizer_sliding_window_size: The sliding window size used for tokenization, with a minimum value of 3 when
            handling requests that include Chinese.
        trust_remote_code ('bool', *optional*, defaults to `False`):
            Whether to allow for custom models in their own modeling files. This option should only be set to `True` for
            repositories you trust and in which you have read the code, as it will execute code present on your local
            machine.
        inference_mode: The name of the inference mode, which is typically parsed from the plugin parameters and does
            not need to be manually specified.
        plugin_params: Plugin parameters used to configure various plugins. The required plugin parameters vary for
            different plugins. For details, please refer to the README documentation or the docstrings in the code of
            each plugin under the path `mindie_llm/text_generator/plugins`.
        local_device_ip: The IP address of the device which the current process is using.
        local_model_instance_id: The ID of the model deployed by the current process. Note that the model IDs deployed
            on different P nodes or D nodes must be unique.
        remote_device_ips: The IP addresses of the devices used by other processes, separated by commas.
        remote_model_instance_ids: The IDs of the models deployed by other processes, and their order must match the
            order of `remote_device_ips`.
        load_tokenizer: Whether to load the tokenizer. The default is `True`. If set to `False`, `mindie_llm` will skip
            loading the tokenizer, which will reduce the service startup time. However, functionalities that require the
            tokenizer, such as the `stop` parameter, will not be available.
        max_position_embeddings: Its meaning is equivalent to `max_seq_len`. However, it is only used to override the
            parameter with the same name during model loading, and is not used to implement framework functionalities.
        max_sequence_length: An alias for `max_position_embeddings`, used by some models.
        tokenizer_path: Used to specify a separate tokenizer path. If not specified, the model path will be used as the
            tokenizer path.
        bos_token_id: Override the `bos_token_id` in the model configuration file.
        eos_token_id: Override the `eos_token_id` in the model configuration file, which is used to determine when
            inference should stop upon encountering certain tokens.
        pad_token_id: Override the `pad_token_id` in the model configuration file, used as a padding token in the cache.
        repetition_penalty: The default value for repetition_penalty when it is not provided in the request.
        frequency_penalty: The default value for frequency_penalty when it is not provided in the request.
        presence_penalty: The default value for presence_penalty when it is not provided in the request.
        temperature: The default value for temperature when it is not provided in the request. If it is set to `0`, the
            sampling feature will be disabled when the temperature parameter is not included in the request.
        top_k: The default value for top_k when it is not provided in the request.
        top_p: The default value for top_p when it is not provided in the request.
        do_sample: The default value for do_sample when it is not provided in the request. If it is set to `False`, the
            sampling feature will be disabled when the temperature parameter is not included in the request.
    """

    # model weight path
    model_id: str

    # memory-related parameters
    block_size: int
    cpu_mem: int
    npu_mem: int

    # device-related parameters
    local_rank: int
    npu_device_id: int
    rank: int
    world_size: int

    # required configuration parameters
    backend_type: BackendType
    max_input_len: int
    max_iter_times: int
    max_prefill_tokens: int
    max_seq_len: int

    # optional configuration parameters with default values
    ignore_eos: Union[bool, str] = False
    model_role: str = "standard"
    num_threads: int = 8
    pd_sepd_npu_port: str = "1234"
    tokenizer_sliding_window_size: int = 3
    trust_remote_code: bool = False

    # plugins configuration parameters
    inference_mode: Optional[InferenceMode] = None
    plugin_params: Optional[str] = None

    # prefilling-decoding separate deployment configurations
    local_device_ip: Optional[str] = None
    local_model_instance_id: Optional[int] = None
    remote_device_ips: Optional[str] = None
    remote_model_instance_ids: Optional[List[int]] = None

    # model loading-related parameters
    load_tokenizer: Optional[bool] = None
    max_position_embeddings: Optional[int] = None
    max_sequence_length: Optional[int] = None
    tokenizer_path: Optional[str] = None

    # special token settings
    bos_token_id: Optional[int] = None
    eos_token_id: Optional[str] = None
    pad_token_id: Optional[int] = None

    # default values of post-processing parameters
    repetition_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    temperature: Optional[float] = None
    top_k: Optional[int] = None
    top_p: Optional[float] = None
    do_sample: Optional[bool] = None

    def __post_init__(self):
        if self.remote_model_instance_ids is None:
            self.remote_model_instance_ids = []
        if self.remote_device_ips is None:
            self.remote_device_ips = ""


@dataclass
class SamplerConfig:
    backend_type: BackendType = None
    npu_id: int = 0
    num_threads: int = 8
    rank: int = 0
    splitfuse_enabled: bool = False
    layerwise_disaggregated: bool = False

    handling_policy: Dict[str, HandlingBackend] = field(default_factory=dict)
    selection_policy: Dict[str, HandlingBackend] = field(default_factory=dict)

    def __post_init__(self):
        if self.backend_type == BackendType.MS:
            self.handling_policy = {
                "repetition_penalty": HandlingBackend.MS,
                "frequency_penalty": HandlingBackend.MS,
                "presence_penalty": HandlingBackend.MS,
                "temperature": HandlingBackend.MS,
                "top_k": HandlingBackend.MS,
                "top_p": HandlingBackend.MS,
            }
            self.selection_policy = {
                "greedy_search": HandlingBackend.MS,
                "sampling": HandlingBackend.MS,
                "top_k_top_p_sampling": HandlingBackend.MS,
                "beam_search": HandlingBackend.MS,
            }
        else:
            self.handling_policy = {
                "repetition_penalty": HandlingBackend.PTA,
                "frequency_penalty": HandlingBackend.PTA,
                "presence_penalty": HandlingBackend.PTA,
                "temperature": HandlingBackend.PTA,
                "top_k": HandlingBackend.PTA,
                "top_p": HandlingBackend.PTA,
            }
            self.selection_policy = {
                "greedy_search": HandlingBackend.PTA,
                "sampling": HandlingBackend.PTA,
                "top_k_top_p_sampling": HandlingBackend.CPU,
                "beam_search": HandlingBackend.PTA,
            }


@dataclass(slots=True)
class ContextParams:
    is_pd_separate: bool = False

    mtp_num_speculative_tokens: int = 0
    mtp_hidden_size: int = 0
    mtp_kv_dtype: Any = None
    mtp_enable: bool = False
    async_infer: bool = False  # original async_inference
    distributed: bool = False
    max_generated_tokens: int = 1

    generator_backend_type: BackendType = BackendType.ATB  # from model_config
    layerwise_disaggregated: bool = False
    layerwise_disaggregated_role_type: str = ""


class SpCpParallelInfo:
    """
    SpCpParallelInfo is used to store the parallel information for sp and cp, including: sp cp size and ranks
    """

    def __init__(self, sp_parallel_info=None, cp_parallel_info=None):
        self.sp_size = 1
        self.sp_rank = 0
        self.cp_size = 1
        self.cp_rank = 0

        if cp_parallel_info and cp_parallel_info.group_size > 1:
            self.cp_size = cp_parallel_info.group_size
            self.cp_rank = cp_parallel_info.rank
        if sp_parallel_info and sp_parallel_info.group_size > 1:
            self.sp_size = sp_parallel_info.group_size
            self.sp_rank = sp_parallel_info.rank
        self.scp_size = self.cp_size * self.sp_size
        self.scp_rank = self.cp_rank * self.sp_size + self.sp_rank


@dataclass
class CacheConfig:
    bos_token_id: int = -1
    cache_size: int = 2048
    eos_token_id: Union[int, List[Union[int, List[int]]]] = 2
    ignore_eos: bool = False
    max_block_size: int = 128
    max_gen_len: int = 512
    max_seq_len: int = 2560
    model_wrapper_config: Any = None
    pad_token_id: int = -1
    pad_rank_id: int = -1
    rank: int = 0
    tokenizer_sliding_window_size: int = 3
    vocab_size: int = -1

    def set_bos_token_id(self, bos_token_id: int) -> None:
        if isinstance(self.bos_token_id, int):
            self.bos_token_id = bos_token_id

    def set_eos_token_id(self, eos_token_id: Union[int, List[Union[int, List[int]]]]) -> None:
        is_valid = True
        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]
        elif isinstance(eos_token_id, list) and eos_token_id:
            for eos_token_id_ins in eos_token_id:
                if isinstance(eos_token_id_ins, int):
                    continue
                elif isinstance(eos_token_id_ins, list) and eos_token_id_ins:
                    for token_id in eos_token_id_ins:
                        if not isinstance(token_id, int):
                            is_valid = False
                else:
                    is_valid = False
        else:
            is_valid = False
        if not is_valid:
            message = (
                "The configured `eos_token_id` has an incorrect format. It must be an integer or a list of "
                "integers. You can modify the `eos_token_id` in the `generation_config.json` file of the model "
                "weights."
            )
            logger.error(message, ErrorCode.TEXT_GENERATOR_EOS_TOKEN_ID_TYPE_INVALID)
            raise ValueError("eos_token_id must be Union[int, List[Union[int, List[int]]]]")
        self.eos_token_id = eos_token_id

    def set_pad_token_id(self, pad_token_id: int) -> None:
        self.pad_token_id = pad_token_id
        if isinstance(self.eos_token_id, int):
            if self.pad_token_id == self.eos_token_id:
                self.pad_token_id = self.vocab_size
        elif isinstance(self.eos_token_id, list):
            if self.pad_token_id in self.eos_token_id:
                self.pad_token_id = self.vocab_size
        self.pad_token_id = int(np.clip(self.pad_token_id, -1, self.vocab_size))


class ResponseConfig(int, Enum):
    EOS = 1
    STOP_STRINGS = 2
    STOP_TOKEN_IDS = 3
    REACH_MAX_SEQ_LEN = 5
    REACH_MAX_OUTPUT_LEN = 6