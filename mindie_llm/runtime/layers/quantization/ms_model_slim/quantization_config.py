# Copyright (c) Huawei Technologies Co., Ltd. 2025-2026. All rights reserved.
# MindIE is licensed under Mulan PSL v2.
# You can use this software according to the terms and conditions of the Mulan PSL v2.
# You may obtain a copy of Mulan PSL v2 at:
#          http://license.coscl.org.cn/MulanPSL2
# THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
# EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
# MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
# See the Mulan PSL v2 for more details.

from typing import Any

import torch

from mindie_llm.runtime.layers.quantization.quantization_config_base import QuantizationConfigBase
from mindie_llm.runtime.layers.quantization.quantization_method_base import QuantizationMethodBase
from mindie_llm.runtime.layers.quantization.ms_model_slim.w8a8 import (
    W8A8PerTensorLinearMethod,
    W8A8PerTokenLinearMethod,
    W8A8MixLinearMethod,
    W8A8MXFP8PerGroupLinearMethod,
)

from mindie_llm.runtime.layers.quantization.ms_model_slim.anti_outlier import AntiOutlierNormMethod
from mindie_llm.runtime.layers.quantization.unquantized import UnquantizedLinearMethod, UnquantizedEmbeddingMethod, \
    UnquantizedNormMethod
from mindie_llm.runtime.layers.embedding.embedding import VocabParallelEmbedding, ParallelLMHead
from mindie_llm.runtime.layers.quantization.ms_model_slim.quant_type import QuantType
from mindie_llm.utils.log.logging import logger


class QuantizationConfig(QuantizationConfigBase):
    """
    Quantization config for weights generated by MindIE Studio
    """
    def __init__(self, config: dict[str, Any]):
        super().__init__()
        self.version = config.get("version", "0.0.0")
        self.model_quant_type = config.get("model_quant_type", None)
        self.quant_descs = config

    @staticmethod
    def get_config_filenames() -> list[str]:
        return ["quant_model_description.json"]

    @classmethod
    def from_config(cls, config: dict[str, Any]) -> QuantizationConfigBase:
        return cls(config)

    def get_quant_type_by_weight_name(self, prefix: str | list[str], suffix: str) -> QuantizationMethodBase | None:
        """
        Retrieve the quantization type for a specific weight parameter.
        Args:
            prefix: Parameter name prefix (string or list of strings for merged weights).
            suffix: Parameter name suffix (e.g., "weight", "bias").
        Returns:
            The quantization type string.
        Raises:
            ValueError: If weight key is not found in descriptions or if merged weights
                       have inconsistent quantization types.
        """
        if isinstance(prefix, str):
            target_key = [f"{prefix}.{suffix}"]
        else:
            target_key = []
            for key in prefix:
                target_key.append(f"{key}.{suffix}")

        quant_type_list = []
        for key in target_key:
            quant_type = self.quant_descs.get(key, "")
            if not quant_type:
                raise ValueError(f"Weight key {target_key} is not found in quantization descriptions "
                                f"extacted from one of {self.get_config_filenames()}.")
            quant_type_list.append(quant_type)
        # NOTE: The current implementation only supports cases where qkv and gateup linears
        # share the same quant type
        if len(set(quant_type_list)) > 1:
            raise ValueError(f"Weight key with prefix {prefix} contains "
                             f"multiple quantization types {quant_type_list}.")
        quant_type = quant_type_list[0]
        return quant_type

    def get_quant_method(
        self, layer: torch.nn.Module, prefix: str | list[str] = ""
    ) -> QuantizationMethodBase | None:
        """
        Retrieve the appropriate quantization method for a given layer.
        Args:
            layer: The PyTorch module (Linear, Norm, Embedding, or MoE layer).
            prefix: Parameter name prefix for the layer.
        Returns:
            The corresponding QuantizationMethodBase instance.
        Note:
            - The default `RMSNorm` operation does not include a bias parameter.
            - If a bias parameter is defined in the quantization description,
              its purpose is to conteract the effects of outliers.
            - For unsupported quantization types, falls back to UnquantizedLinearMethod with a warning.
        """
        from mindie_llm.runtime.layers.normalization import RMSNorm
        if isinstance(layer, RMSNorm):
            if f"{prefix}.bias" in self.quant_descs:
                return AntiOutlierNormMethod()
            return UnquantizedNormMethod()

        quant_type = self.get_quant_type_by_weight_name(prefix, "weight")
        from mindie_llm.runtime.layers.linear.linear import LinearBase
        if isinstance(layer, LinearBase):
            if quant_type == QuantType.W8A8_MIX and self.model_quant_type != QuantType.W8A8_MIX:
                quant_type = self.model_quant_type
            quant_type_method_cls_map = {
                QuantType.FLOAT: UnquantizedLinearMethod,
                QuantType.W8A8: W8A8PerTensorLinearMethod,
                QuantType.W8A8_DYNAMIC: W8A8PerTokenLinearMethod,
                QuantType.W8A8_MIX: W8A8MixLinearMethod,
                QuantType.W8A8_MXFP8: W8A8MXFP8PerGroupLinearMethod,
            }
            quant_method_cls = quant_type_method_cls_map.get(quant_type)
            if quant_method_cls is None:
                logger.warning(f"Quantization type {quant_type} is not found in all the LinearMethods. "
                               f"Use `UnquantizedLinearMethod` instead.")
                return UnquantizedLinearMethod()
            else:
                return quant_method_cls()
        elif isinstance(layer, ParallelLMHead):
            return UnquantizedLinearMethod()
        elif isinstance(layer, VocabParallelEmbedding):
            return UnquantizedEmbeddingMethod()

        return None
